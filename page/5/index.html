<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"/>




  <link href="//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">



<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=0.5.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=0.5.0" />






<meta name="description" content="当才华撑不起野心时，应该静下心来学习；当能力驾驭不了目标时，应该沉下心来历练。">
<meta property="og:type" content="website">
<meta property="og:title" content="1CSH1">
<meta property="og:url" content="https://1csh1.github.io/page/5/index.html">
<meta property="og:site_name" content="1CSH1">
<meta property="og:description" content="当才华撑不起野心时，应该静下心来学习；当能力驾驭不了目标时，应该沉下心来历练。">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="1CSH1">
<meta name="twitter:description" content="当才华撑不起野心时，应该静下心来学习；当能力驾驭不了目标时，应该沉下心来历练。">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>

  <title> 1CSH1 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  





  <script type="text/javascript">
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=55186219";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>






  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">1CSH1</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">当才华撑不起野心时，应该静下心来学习；当能力驾驭不了目标时，应该沉下心来历练。</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home fa-fw"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-archive fa-fw"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-th fa-fw"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-tags fa-fw"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/05/24/R-导入Excel数据/" itemprop="url">
                  R-导入Excel数据
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-05-24T16:23:30+08:00" content="2016-05-24">
              2016-05-24
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/R/" itemprop="url" rel="index">
                    <span itemprop="name">R</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/05/24/R-导入Excel数据/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/05/24/R-导入Excel数据/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/05/24/R-导入Excel数据/" class="leancloud_visitors" data-flag-title="R-导入Excel数据">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="u5BFC_u51652007_u524D_u7684Excel"><a href="#u5BFC_u51652007_u524D_u7684Excel" class="headerlink" title="导入2007前的Excel"></a>导入2007前的Excel</h3><h4 id="u4E0B_u8F7D_u5B89_u88C5RODBC_u5305"><a href="#u4E0B_u8F7D_u5B89_u88C5RODBC_u5305" class="headerlink" title="下载安装RODBC包"></a>下载安装RODBC包</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&#34;RODBC&#34;)</span><br></pre></td></tr></table></figure>
<h4 id="Window_32-bit__u5BFC_u5165_u6570_u636E"><a href="#Window_32-bit__u5BFC_u5165_u6570_u636E" class="headerlink" title="Window 32-bit 导入数据"></a>Window 32-bit 导入数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">library(RODBC)&#10;channel &#60;- odbcConnectExcel(&#34;myfile.xls&#34;)&#10;mydataframe &#60;- sqlFetch(channel, &#34;mysheet&#34;)&#10;odbcClose(channel)</span><br></pre></td></tr></table></figure>
<h4 id="Window_64-bit__u5BFC_u5165_u6570_u636E"><a href="#Window_64-bit__u5BFC_u5165_u6570_u636E" class="headerlink" title="Window 64-bit 导入数据"></a>Window 64-bit 导入数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">library(RODBC)&#10;channel &#60;- odbcConnectExcel2007(&#34;myfile.xls&#34;)&#10;mydataframe &#60;- sqlFetch(channel, &#34;mysheet&#34;)&#10;odbcClose(channel)</span><br></pre></td></tr></table></figure>
<h3 id="u5BFC_u51652007_Excel"><a href="#u5BFC_u51652007_Excel" class="headerlink" title="导入2007 Excel"></a>导入2007 Excel</h3><h4 id="u4E0B_u8F7D_u5B89_u88C5xlsx_u5305"><a href="#u4E0B_u8F7D_u5B89_u88C5xlsx_u5305" class="headerlink" title="下载安装xlsx包"></a>下载安装xlsx包</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&#34;xlsx&#34;)</span><br></pre></td></tr></table></figure>
<h4 id="u5BFC_u5165_u6570_u636E"><a href="#u5BFC_u5165_u6570_u636E" class="headerlink" title="导入数据"></a>导入数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">library(xlsx)&#10;workbook &#60;- &#34;abc.xlsx&#34;&#10;mydataframe &#60;- read.xlsx(workbook, 1)&#10;mydataframe #&#26174;&#31034;Excel&#25968;&#25454;</span><br></pre></td></tr></table></figure>
<h4 id="u663E_u793A_u5BFC_u5165_u6570_u636E"><a href="#u663E_u793A_u5BFC_u5165_u6570_u636E" class="headerlink" title="显示导入数据"></a>显示导入数据</h4><p><img src="https://1csh1.github.io/img/R-导入Excel数据/显示数据.jpg" alt="显示数据"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/04/21/Flume集群搭建/" itemprop="url">
                  Flume集群搭建
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-04-21T18:20:43+08:00" content="2016-04-21">
              2016-04-21
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Flume/" itemprop="url" rel="index">
                    <span itemprop="name">Flume</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/04/21/Flume集群搭建/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/04/21/Flume集群搭建/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/04/21/Flume集群搭建/" class="leancloud_visitors" data-flag-title="Flume集群搭建">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="u6982_u5FF5"><a href="#u6982_u5FF5" class="headerlink" title="概念"></a>概念</h3><p>集群的意思是多台机器，最少有2台机器，一台机器从数据源中获取数据，将数据传送到另一台机器上，然后输出。接下来就要实现Flume集群搭建。文中的集群如下图所示。<br><img src="https://1csh1.github.io/img/Flume集群搭建/架构.jpg" alt="架构"><br>这里我们需要2台机器，node1作为push推送数据，node2作为pull获取数据后显示出来。</p>
<h3 id="u914D_u7F6Epull-conf"><a href="#u914D_u7F6Epull-conf" class="headerlink" title="配置pull.conf"></a>配置pull.conf</h3><p>【在node2机器上操作】<br>在conf目录下创建pull.conf文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">touch pull.conf</span><br></pre></td></tr></table></figure></p>
<p>编辑pull.conf<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#&#27719;&#24635;&#25968;&#25454;&#20195;&#29702;&#30340;&#37197;&#32622;&#25991;&#20214;pull.conf&#10;#Name the components on this agent&#10;a1.sources= r1&#10;a1.sinks= k1&#10;a1.channels= c1&#10; &#10;#Describe/configure the source&#10;a1.sources.r1.type= avro&#10;a1.sources.r1.channels= c1&#10;a1.sources.r1.bind= node2&#10;a1.sources.r1.port= 44444&#10; &#10;#Describe the sink&#10;a1.sinks.k1.type= logger&#10;a1.sinks.k1.channel = c1&#10; &#10;#Use a channel which buffers events in memory&#10;a1.channels.c1.type= memory&#10;a1.channels.c1.keep-alive= 10&#10;a1.channels.c1.capacity= 100000&#10;a1.channels.c1.transactionCapacity= 100000</span><br></pre></td></tr></table></figure></p>
<h3 id="u914D_u7F6Epush-conf"><a href="#u914D_u7F6Epush-conf" class="headerlink" title="配置push.conf"></a>配置push.conf</h3><p>【在node1机器上操作】<br>在conf目录下创建push.conf文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">touch push.conf</span><br></pre></td></tr></table></figure></p>
<p>编辑push.conf<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#&#25512;&#25968;&#25454;&#20195;&#29702;&#30340;&#37197;&#32622;&#25991;&#20214;push.conf&#10;#Name the components on this agent&#10;a2.sources= r1&#10;a2.sinks= k1&#10;a2.channels= c1&#10; &#10;#Describe/configure the source&#10;a2.sources.r1.type= spooldir&#10;a2.sources.r1.spoolDir= /csh/hadoop/flume/logs&#10;a2.sources.r1.channels= c1&#10; &#10;#Use a channel which buffers events in memory&#10;a2.channels.c1.type= memory&#10;a2.channels.c1.keep-alive= 10&#10;a2.channels.c1.capacity= 100000&#10;a2.channels.c1.transactionCapacity= 100000&#10; &#10;#Describe/configure the source&#10;a2.sinks.k1.type= avro&#10;a2.sinks.k1.channel= c1&#10;a2.sinks.k1.hostname= node2&#10;a2.sinks.k1.port= 44444</span><br></pre></td></tr></table></figure></p>
<h3 id="u521B_u5EFAspoolDir_u76EE_u5F55"><a href="#u521B_u5EFAspoolDir_u76EE_u5F55" class="headerlink" title="创建spoolDir目录"></a>创建spoolDir目录</h3><p>【在node1中进行该操作】<br>根据push.conf中的配置 a2.sources.r1.spoolDir参数，创建目录，如果不先创建目录，则启动时会报错<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /csh/hadoop/flume/logs</span><br></pre></td></tr></table></figure></p>
<h3 id="u542F_u52A8_u4F5C_u4E3Apull_u7684_u4E3B_u673A"><a href="#u542F_u52A8_u4F5C_u4E3Apull_u7684_u4E3B_u673A" class="headerlink" title="启动作为pull的主机"></a>启动作为pull的主机</h3><p>【本文为node2主机】<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node2 flume]# flume-ng agent -c conf -f conf/pull.conf -n a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></p>
<p>显示如下信息则为启动成功</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2016-04-20 00:08:15,550 (conf-file-poller-0) [INFO - org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:133)] Reloading configuration file:conf/pull.conf&#10;2016-04-20 00:08:15,573 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:931)] Added sinks: k1 Agent: a1&#10;2016-04-20 00:08:15,573 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)] Processing:k1&#10;2016-04-20 00:08:15,574 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)] Processing:k1&#10;2016-04-20 00:08:15,621 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration.validateConfiguration(FlumeConfiguration.java:141)] Post-validation flume configuration contains configuration for agents: [a1]&#10;2016-04-20 00:08:15,622 (conf-file-poller-0) [INFO - org.apache.flume.node.AbstractConfigurationProvider.loadChannels(AbstractConfigurationProvider.java:145)] Creating channels&#10;2016-04-20 00:08:15,658 (conf-file-poller-0) [INFO - org.apache.flume.channel.DefaultChannelFactory.create(DefaultChannelFactory.java:42)] Creating instance of channel c1 type memory&#10;2016-04-20 00:08:15,672 (conf-file-poller-0) [INFO - org.apache.flume.node.AbstractConfigurationProvider.loadChannels(AbstractConfigurationProvider.java:200)] Created channel c1&#10;2016-04-20 00:08:15,677 (conf-file-poller-0) [INFO - org.apache.flume.source.DefaultSourceFactory.create(DefaultSourceFactory.java:41)] Creating instance of source r1, type avro&#10;2016-04-20 00:08:15,732 (conf-file-poller-0) [INFO - org.apache.flume.sink.DefaultSinkFactory.create(DefaultSinkFactory.java:42)] Creating instance of sink: k1, type: logger&#10;2016-04-20 00:08:15,735 (conf-file-poller-0) [INFO - org.apache.flume.node.AbstractConfigurationProvider.getConfiguration(AbstractConfigurationProvider.java:114)] Channel c1 connected to [r1, k1]&#10;2016-04-20 00:08:15,750 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:138)] Starting new configuration:&#123; sourceRunners:&#123;r1=EventDrivenSourceRunner: &#123; source:Avro source r1: &#123; bindAddress: node2, port: 44444 &#125; &#125;&#125; sinkRunners:&#123;k1=SinkRunner: &#123; policy:org.apache.flume.sink.DefaultSinkProcessor@ea5ba80 counterGroup:&#123; name:null counters:&#123;&#125; &#125; &#125;&#125; channels:&#123;c1=org.apache.flume.channel.MemoryChannel&#123;name: c1&#125;&#125; &#125;&#10;2016-04-20 00:08:15,782 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:145)] Starting Channel c1&#10;2016-04-20 00:08:15,784 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:160)] Waiting for channel: c1 to start. Sleeping for 500 ms&#10;2016-04-20 00:08:15,897 (lifecycleSupervisor-1-2) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:120)] Monitored counter group for type: CHANNEL, name: c1: Successfully registered new MBean.&#10;2016-04-20 00:08:15,901 (lifecycleSupervisor-1-2) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:96)] Component type: CHANNEL, name: c1 started&#10;2016-04-20 00:08:16,285 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:173)] Starting Sink k1&#10;2016-04-20 00:08:16,288 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:184)] Starting Source r1&#10;2016-04-20 00:08:16,298 (lifecycleSupervisor-1-2) [INFO - org.apache.flume.source.AvroSource.start(AvroSource.java:228)] Starting Avro source r1: &#123; bindAddress: node2, port: 44444 &#125;...&#10;2016-04-20 00:08:16,951 (lifecycleSupervisor-1-2) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:120)] Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.&#10;2016-04-20 00:08:16,952 (lifecycleSupervisor-1-2) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:96)] Component type: SOURCE, name: r1 started&#10;2016-04-20 00:08:16,959 (lifecycleSupervisor-1-2) [INFO - org.apache.flume.source.AvroSource.start(AvroSource.java:253)] Avro source r1 started.</span><br></pre></td></tr></table></figure>
<h3 id="u542F_u52A8_u4F5C_u4E3Apush_u7684_u4E3B_u673A"><a href="#u542F_u52A8_u4F5C_u4E3Apush_u7684_u4E3B_u673A" class="headerlink" title="启动作为push的主机"></a>启动作为push的主机</h3><p>【本文为node1主机】<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 flume]# flume-ng agent -n a2 -c conf -f conf/push.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></p>
<p>显示如下信息则为启动成功<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2016-04-20 00:11:58,196 (conf-file-poller-0) [INFO - org.apache.flume.node.AbstractConfigurationProvider.getConfiguration(AbstractConfigurationProvider.java:114)] Channel c1 connected to [r1, k1]&#10;2016-04-20 00:11:58,226 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:138)] Starting new configuration:&#123; sourceRunners:&#123;r1=EventDrivenSourceRunner: &#123; source:Spool Directory source r1: &#123; spoolDir: /csh/hadoop/flume/logs &#125; &#125;&#125; sinkRunners:&#123;k1=SinkRunner: &#123; policy:org.apache.flume.sink.DefaultSinkProcessor@6b089e25 counterGroup:&#123; name:null counters:&#123;&#125; &#125; &#125;&#125; channels:&#123;c1=org.apache.flume.channel.MemoryChannel&#123;name: c1&#125;&#125; &#125;&#10;2016-04-20 00:11:58,236 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:145)] Starting Channel c1&#10;2016-04-20 00:11:58,360 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:120)] Monitored counter group for type: CHANNEL, name: c1: Successfully registered new MBean.&#10;2016-04-20 00:11:58,361 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:96)] Component type: CHANNEL, name: c1 started&#10;2016-04-20 00:11:58,362 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:173)] Starting Sink k1&#10;2016-04-20 00:11:58,369 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:184)] Starting Source r1&#10;2016-04-20 00:11:58,372 (lifecycleSupervisor-1-4) [INFO - org.apache.flume.source.SpoolDirectorySource.start(SpoolDirectorySource.java:78)] SpoolDirectorySource source starting with directory: /csh/hadoop/flume/logs&#10;2016-04-20 00:11:58,388 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.sink.AbstractRpcSink.start(AbstractRpcSink.java:289)] Starting RpcSink k1 &#123; host: node2, port: 44444 &#125;...&#10;2016-04-20 00:11:58,409 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:120)] Monitored counter group for type: SINK, name: k1: Successfully registered new MBean.&#10;2016-04-20 00:11:58,409 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:96)] Component type: SINK, name: k1 started&#10;2016-04-20 00:11:58,409 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.sink.AbstractRpcSink.createConnection(AbstractRpcSink.java:206)] Rpc sink k1: Building RpcClient with hostname: node2, port: 44444&#10;2016-04-20 00:11:58,410 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.sink.AvroSink.initializeRpcClient(AvroSink.java:126)] Attempting to create Avro Rpc client.&#10;2016-04-20 00:11:58,458 (lifecycleSupervisor-1-0) [WARN - org.apache.flume.api.NettyAvroRpcClient.configure(NettyAvroRpcClient.java:634)] Using default maxIOWorkers&#10;2016-04-20 00:11:58,536 (lifecycleSupervisor-1-4) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:120)] Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.&#10;2016-04-20 00:11:58,536 (lifecycleSupervisor-1-4) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:96)] Component type: SOURCE, name: r1 started&#10;2016-04-20 00:11:59,263 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.sink.AbstractRpcSink.start(AbstractRpcSink.java:303)] Rpc sink k1 started.</span><br></pre></td></tr></table></figure></p>
<p>这时pull主机【本文为node2】输出信息表示连接成功<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2016-04-20 00:11:58,875 (New I/O server boss #1 ([id: 0x71ba9ce2, /192.168.161.12:44444])) [INFO - org.apache.avro.ipc.NettyServer$NettyServerAvroHandler.handleUpstream(NettyServer.java:171)] [id: 0x7d9299a9, /192.168.161.11:44003 =&#62; /192.168.161.12:44444] OPEN&#10;2016-04-20 00:11:58,880 (New I/O  worker #1) [INFO - org.apache.avro.ipc.NettyServer$NettyServerAvroHandler.handleUpstream(NettyServer.java:171)] [id: 0x7d9299a9, /192.168.161.11:44003 =&#62; /192.168.161.12:44444] BOUND: /192.168.161.12:44444&#10;2016-04-20 00:11:58,884 (New I/O  worker #1) [INFO - org.apache.avro.ipc.NettyServer$NettyServerAvroHandler.handleUpstream(NettyServer.java:171)] [id: 0x7d9299a9, /192.168.161.11:44003 =&#62; /192.168.161.12:44444] CONNECTED: /192.168.161.11:44003</span><br></pre></td></tr></table></figure></p>
<h3 id="u6D4B_u8BD5"><a href="#u6D4B_u8BD5" class="headerlink" title="测试"></a>测试</h3><p>在push主机中【本文为node1】的spoolDir目录【本文为/csh/hadoop/flume/logs】中创建test.log<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi test.log&#10;# &#36755;&#20837;&#20869;&#23481; hello flume</span><br></pre></td></tr></table></figure></p>
<p>这时push主机【本文为node1】中命令行输出如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2016-04-20 00:13:09,274 (pool-4-thread-1) [INFO - org.apache.flume.client.avro.ReliableSpoolingFileEventReader.readEvents(ReliableSpoolingFileEventReader.java:258)] Last read took us just up to a file boundary. Rolling to the next file, if there is one.&#10;2016-04-20 00:13:09,275 (pool-4-thread-1) [INFO - org.apache.flume.client.avro.ReliableSpoolingFileEventReader.rollCurrentFile(ReliableSpoolingFileEventReader.java:348)] Preparing to move file /csh/hadoop/flume/logs/test.log to /csh/hadoop/flume/logs/test.log.COMPLETED</span><br></pre></td></tr></table></figure></p>
<p>pull主机【本文为node2】中命令行输出如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2016-04-20 00:13:21,344 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:94)] Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F 20 66 6C 75 6D 65                hello flume &#125;</span><br></pre></td></tr></table></figure></p>
<p>证明Flume集群搭建成功</p>
<p>我们可以发现test.log被改名为test.log.COMPLETED</p>

          
        
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/04/21/Flume单机安装与配置/" itemprop="url">
                  Flume单机安装与配置
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-04-21T18:15:35+08:00" content="2016-04-21">
              2016-04-21
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Flume/" itemprop="url" rel="index">
                    <span itemprop="name">Flume</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/04/21/Flume单机安装与配置/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/04/21/Flume单机安装与配置/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/04/21/Flume单机安装与配置/" class="leancloud_visitors" data-flag-title="Flume单机安装与配置">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="u4E0B_u8F7D"><a href="#u4E0B_u8F7D" class="headerlink" title="下载"></a>下载</h3><p><a href="https://1csh1.github.io/file/Flume单机安装与配置/apache-flume-1.6.0-bin.tar.gz">apache-flume-1.6.0-bin.tar.gz</a></p>
<h3 id="u89E3_u538B"><a href="#u89E3_u538B" class="headerlink" title="解压"></a>解压</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-flume-1.6.0-bin.tar.gz</span><br></pre></td></tr></table></figure>
<h3 id="u914D_u7F6E"><a href="#u914D_u7F6E" class="headerlink" title="配置"></a>配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp conf/flume-conf.properties.template conf/flume-conf.properties&#10;cp conf/flume-env.sh.template conf/flume-env.sh</span><br></pre></td></tr></table></figure>
<p>flume-env.sh<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># Enviroment variables can be set here.&#10;export JAVA_HOME=/csh/link/jdk&#10;&#10;# Give Flume more memory and pre-allocate, enable remote monitoring via JMX&#10;# export JAVA_OPTS=&#34;-Xms100m -Xmx2000m -Dcom.sun.management.jmxremote&#34;&#10;&#10;# Note that the Flume conf directory is always included in the classpath.&#10;FLUME_CLASSPATH=/csh/link/flume/lib</span><br></pre></td></tr></table></figure></p>
<h3 id="u5355_u673A_u914D_u7F6E"><a href="#u5355_u673A_u914D_u7F6E" class="headerlink" title="单机配置"></a>单机配置</h3><p>在conf目录下创建single-node.conf文件，并将下面内容复制粘贴</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># example.conf: A single-node Flume configuration&#10;&#10;# Name the components on this agent&#10;a1.sources = r1&#10;a1.sinks = k1&#10;a1.channels = c1&#10;&#10;# Describe/configure the source&#10;a1.sources.r1.type = netcat&#10;a1.sources.r1.bind = localhost&#10;a1.sources.r1.port = 44444&#10;&#10;# Describe the sink&#10;a1.sinks.k1.type = logger&#10;&#10;# Use a channel which buffers events in memory&#10;a1.channels.c1.type = memory&#10;a1.channels.c1.capacity = 1000&#10;a1.channels.c1.transactionCapacity = 100&#10;&#10;# Bind the source and sink to the channel&#10;a1.sources.r1.channels = c1&#10;a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
<h3 id="u8FD0_u884C"><a href="#u8FD0_u884C" class="headerlink" title="运行"></a>运行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent --conf conf --conf-file conf/single-node.conf --name a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>
<p>命令行输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Info: Sourcing environment configuration script /csh/link/flume/conf/flume-env.sh&#10;Info: Including Hadoop libraries found via (/csh/link/hadoop/bin/hadoop) for HDFS access&#10;Info: Excluding /csh/software/hadoop-2.7.2/share/hadoop/common/lib/slf4j-api-1.7.10.jar from classpath&#10;Info: Excluding /csh/software/hadoop-2.7.2/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar from classpath&#10;Info: Including HBASE libraries found via (/csh/link/hbase/bin/hbase) for HBASE access&#10;Info: Excluding /csh/link/hbase/lib/slf4j-api-1.7.7.jar from classpath&#10;Info: Excluding /csh/link/hbase/lib/slf4j-log4j12-1.7.5.jar from classpath&#10;Info: Excluding /csh/software/hadoop-2.7.2/share/hadoop/common/lib/slf4j-api-1.7.10.jar from classpath&#10;Info: Excluding /csh/software/hadoop-2.7.2/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar from classpath&#10;Info: Including Hive libraries found via (/csh/link/hive) for Hive access&#10;+ exec /csh/link/jdk/bin/java -Xmx20m -Dflume.root.logger=INFO,console -cp &#39;/csh/link/flume/conf:/csh/link/flume/lib/*:/csh/link/flume/lib:/csh/software/hadoop-2.7.2/etc/hadoop:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/activation-1.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/asm-3.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/avro-1.7.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-cli-1.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-codec-1.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-collections-3.2.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-compress-1.4.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-configuration-1.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-digester-1.8.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-httpclient-3.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-io-2.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-lang-2.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-net-3.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/curator-client-2.7.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/curator-framework-2.7.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/gson-2.2.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/guava-11.0.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/hadoop-annotations-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/hadoop-auth-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/hamcrest-core-1.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/httpclient-4.2.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/httpcore-4.2.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jersey-core-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jersey-json-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jersey-server-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jets3t-0.9.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jettison-1.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jetty-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jetty-util-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jsch-0.1.42.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jsp-api-2.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jsr305-3.0.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/junit-4.11.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/log4j-1.2.17.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/mockito-all-1.8.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/netty-3.6.2.Final.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/paranamer-2.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/servlet-api-2.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/stax-api-1.0-2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/xmlenc-0.52.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/xz-1.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/zookeeper-3.4.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/hadoop-common-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/hadoop-common-2.7.2-tests.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/hadoop-nfs-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/jdiff:/csh/software/hadoop-2.7.2/share/hadoop/common/lib:/csh/software/hadoop-2.7.2/share/hadoop/common/sources:/csh/software/hadoop-2.7.2/share/hadoop/common/templates:/csh/software/hadoop-2.7.2/share/hadoop/hdfs:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/asm-3.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-io-2.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/guava-11.0.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/hadoop-hdfs-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/hadoop-hdfs-2.7.2-tests.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/jdiff:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/sources:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/templates:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/webapps:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/activation-1.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/asm-3.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-cli-1.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-codec-1.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-io-2.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/guava-11.0.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/guice-3.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/javax.inject-1.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jersey-client-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jersey-core-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jersey-json-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jersey-server-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jettison-1.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jetty-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/log4j-1.2.17.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/servlet-api-2.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/xz-1.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-api-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-client-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-common-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-registry-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-common-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib:/csh/software/hadoop-2.7.2/share/hadoop/yarn/sources:/csh/software/hadoop-2.7.2/share/hadoop/yarn/test:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/asm-3.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/guice-3.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/javax.inject-1.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/xz-1.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib-examples:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/sources:/csh/link/hadoop/contrib/capacity-scheduler/*.jar:/csh/link/hbase/conf:/csh/link/jdk/lib/tools.jar:/csh/link/hbase:/csh/link/hbase/lib/activation-1.1.jar:/csh/link/hbase/lib/antisamy-1.4.3.jar:/csh/link/hbase/lib/aopalliance-1.0.jar:/csh/link/hbase/lib/apacheds-i18n-2.0.0-M15.jar:/csh/link/hbase/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/csh/link/hbase/lib/api-asn1-api-1.0.0-M20.jar:/csh/link/hbase/lib/api-util-1.0.0-M20.jar:/csh/link/hbase/lib/asm-3.1.jar:/csh/link/hbase/lib/avro-1.7.4.jar:/csh/link/hbase/lib/batik-css-1.7.jar:/csh/link/hbase/lib/batik-ext-1.7.jar:/csh/link/hbase/lib/batik-util-1.7.jar:/csh/link/hbase/lib/bsh-core-2.0b4.jar:/csh/link/hbase/lib/commons-beanutils-1.7.0.jar:/csh/link/hbase/lib/commons-beanutils-core-1.7.0.jar:/csh/link/hbase/lib/commons-cli-1.2.jar:/csh/link/hbase/lib/commons-codec-1.9.jar:/csh/link/hbase/lib/commons-collections-3.2.2.jar:/csh/link/hbase/lib/commons-compress-1.4.1.jar:/csh/link/hbase/lib/commons-configuration-1.6.jar:/csh/link/hbase/lib/commons-daemon-1.0.13.jar:/csh/link/hbase/lib/commons-digester-1.8.jar:/csh/link/hbase/lib/commons-el-1.0.jar:/csh/link/hbase/lib/commons-fileupload-1.2.jar:/csh/link/hbase/lib/commons-httpclient-3.1.jar:/csh/link/hbase/lib/commons-io-2.4.jar:/csh/link/hbase/lib/commons-lang-2.6.jar:/csh/link/hbase/lib/commons-logging-1.2.jar:/csh/link/hbase/lib/commons-math-2.2.jar:/csh/link/hbase/lib/commons-math3-3.1.1.jar:/csh/link/hbase/lib/commons-net-3.1.jar:/csh/link/hbase/lib/disruptor-3.3.0.jar:/csh/link/hbase/lib/esapi-2.1.0.jar:/csh/link/hbase/lib/findbugs-annotations-1.3.9-1.jar:/csh/link/hbase/lib/guava-12.0.1.jar:/csh/link/hbase/lib/guice-3.0.jar:/csh/link/hbase/lib/guice-servlet-3.0.jar:/csh/link/hbase/lib/hadoop-annotations-2.5.1.jar:/csh/link/hbase/lib/hadoop-auth-2.5.1.jar:/csh/link/hbase/lib/hadoop-client-2.5.1.jar:/csh/link/hbase/lib/hadoop-common-2.5.1.jar:/csh/link/hbase/lib/hadoop-hdfs-2.5.1.jar:/csh/link/hbase/lib/hadoop-mapreduce-client-app-2.5.1.jar:/csh/link/hbase/lib/hadoop-mapreduce-client-common-2.5.1.jar:/csh/link/hbase/lib/hadoop-mapreduce-client-core-2.5.1.jar:/csh/link/hbase/lib/hadoop-mapreduce-client-jobclient-2.5.1.jar:/csh/link/hbase/lib/hadoop-mapreduce-client-shuffle-2.5.1.jar:/csh/link/hbase/lib/hadoop-yarn-api-2.5.1.jar:/csh/link/hbase/lib/hadoop-yarn-client-2.5.1.jar:/csh/link/hbase/lib/hadoop-yarn-common-2.5.1.jar:/csh/link/hbase/lib/hadoop-yarn-server-common-2.5.1.jar:/csh/link/hbase/lib/hbase-annotations-1.1.4.jar:/csh/link/hbase/lib/hbase-annotations-1.1.4-tests.jar:/csh/link/hbase/lib/hbase-client-1.1.4.jar:/csh/link/hbase/lib/hbase-common-1.1.4.jar:/csh/link/hbase/lib/hbase-common-1.1.4-tests.jar:/csh/link/hbase/lib/hbase-examples-1.1.4.jar:/csh/link/hbase/lib/hbase-hadoop2-compat-1.1.4.jar:/csh/link/hbase/lib/hbase-hadoop-compat-1.1.4.jar:/csh/link/hbase/lib/hbase-it-1.1.4.jar:/csh/link/hbase/lib/hbase-it-1.1.4-tests.jar:/csh/link/hbase/lib/hbase-prefix-tree-1.1.4.jar:/csh/link/hbase/lib/hbase-procedure-1.1.4.jar:/csh/link/hbase/lib/hbase-protocol-1.1.4.jar:/csh/link/hbase/lib/hbase-resource-bundle-1.1.4.jar:/csh/link/hbase/lib/hbase-rest-1.1.4.jar:/csh/link/hbase/lib/hbase-server-1.1.4.jar:/csh/link/hbase/lib/hbase-server-1.1.4-tests.jar:/csh/link/hbase/lib/hbase-shell-1.1.4.jar:/csh/link/hbase/lib/hbase-thrift-1.1.4.jar:/csh/link/hbase/lib/htrace-core-3.1.0-incubating.jar:/csh/link/hbase/lib/httpclient-4.2.5.jar:/csh/link/hbase/lib/httpcore-4.1.3.jar:/csh/link/hbase/lib/jackson-core-asl-1.9.13.jar:/csh/link/hbase/lib/jackson-jaxrs-1.9.13.jar:/csh/link/hbase/lib/jackson-mapper-asl-1.9.13.jar:/csh/link/hbase/lib/jackson-xc-1.9.13.jar:/csh/link/hbase/lib/jamon-runtime-2.3.1.jar:/csh/link/hbase/lib/jasper-compiler-5.5.23.jar:/csh/link/hbase/lib/jasper-runtime-5.5.23.jar:/csh/link/hbase/lib/javax.inject-1.jar:/csh/link/hbase/lib/java-xmlbuilder-0.4.jar:/csh/link/hbase/lib/jaxb-api-2.2.2.jar:/csh/link/hbase/lib/jaxb-impl-2.2.3-1.jar:/csh/link/hbase/lib/jcodings-1.0.8.jar:/csh/link/hbase/lib/jersey-client-1.9.jar:/csh/link/hbase/lib/jersey-core-1.9.jar:/csh/link/hbase/lib/jersey-guice-1.9.jar:/csh/link/hbase/lib/jersey-json-1.9.jar:/csh/link/hbase/lib/jersey-server-1.9.jar:/csh/link/hbase/lib/jets3t-0.9.0.jar:/csh/link/hbase/lib/jettison-1.3.3.jar:/csh/link/hbase/lib/jetty-6.1.26.jar:/csh/link/hbase/lib/jetty-sslengine-6.1.26.jar:/csh/link/hbase/lib/jetty-util-6.1.26.jar:/csh/link/hbase/lib/joni-2.1.2.jar:/csh/link/hbase/lib/jruby-complete-1.6.8.jar:/csh/link/hbase/lib/jsch-0.1.42.jar:/csh/link/hbase/lib/jsp-2.1-6.1.14.jar:/csh/link/hbase/lib/jsp-api-2.1-6.1.14.jar:/csh/link/hbase/lib/jsr305-1.3.9.jar:/csh/link/hbase/lib/junit-4.12.jar:/csh/link/hbase/lib/leveldbjni-all-1.8.jar:/csh/link/hbase/lib/libthrift-0.9.0.jar:/csh/link/hbase/lib/log4j-1.2.17.jar:/csh/link/hbase/lib/metrics-core-2.2.0.jar:/csh/link/hbase/lib/nekohtml-1.9.12.jar:/csh/link/hbase/lib/netty-3.2.4.Final.jar:/csh/link/hbase/lib/netty-all-4.0.23.Final.jar:/csh/link/hbase/lib/paranamer-2.3.jar:/csh/link/hbase/lib/protobuf-java-2.5.0.jar:/csh/link/hbase/lib/servlet-api-2.5-6.1.14.jar:/csh/link/hbase/lib/servlet-api-2.5.jar:/csh/link/hbase/lib/snappy-java-1.0.4.1.jar:/csh/link/hbase/lib/spymemcached-2.11.6.jar:/csh/link/hbase/lib/xalan-2.7.0.jar:/csh/link/hbase/lib/xml-apis-1.3.03.jar:/csh/link/hbase/lib/xml-apis-ext-1.3.04.jar:/csh/link/hbase/lib/xmlenc-0.52.jar:/csh/link/hbase/lib/xom-1.2.5.jar:/csh/link/hbase/lib/xz-1.0.jar:/csh/link/hbase/lib/zookeeper-3.4.6.jar:/csh/software/hadoop-2.7.2/etc/hadoop:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/activation-1.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/asm-3.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/avro-1.7.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-cli-1.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-codec-1.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-collections-3.2.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-compress-1.4.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-configuration-1.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-digester-1.8.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-httpclient-3.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-io-2.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-lang-2.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-net-3.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/curator-client-2.7.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/curator-framework-2.7.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/gson-2.2.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/guava-11.0.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/hadoop-annotations-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/hadoop-auth-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/hamcrest-core-1.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/httpclient-4.2.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/httpcore-4.2.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jersey-core-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jersey-json-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jersey-server-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jets3t-0.9.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jettison-1.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jetty-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jetty-util-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jsch-0.1.42.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jsp-api-2.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jsr305-3.0.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/junit-4.11.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/log4j-1.2.17.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/mockito-all-1.8.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/netty-3.6.2.Final.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/paranamer-2.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/servlet-api-2.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/stax-api-1.0-2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/xmlenc-0.52.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/xz-1.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/zookeeper-3.4.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/hadoop-common-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/hadoop-common-2.7.2-tests.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/hadoop-nfs-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/jdiff:/csh/software/hadoop-2.7.2/share/hadoop/common/lib:/csh/software/hadoop-2.7.2/share/hadoop/common/sources:/csh/software/hadoop-2.7.2/share/hadoop/common/templates:/csh/software/hadoop-2.7.2/share/hadoop/hdfs:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/asm-3.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-io-2.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/guava-11.0.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/hadoop-hdfs-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/hadoop-hdfs-2.7.2-tests.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/jdiff:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/sources:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/templates:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/webapps:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/activation-1.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/asm-3.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-cli-1.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-codec-1.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-io-2.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/guava-11.0.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/guice-3.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/javax.inject-1.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jersey-client-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jersey-core-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jersey-json-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jersey-server-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jettison-1.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jetty-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/log4j-1.2.17.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/servlet-api-2.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/xz-1.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-api-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-client-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-common-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-registry-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-common-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib:/csh/software/hadoop-2.7.2/share/hadoop/yarn/sources:/csh/software/hadoop-2.7.2/share/hadoop/yarn/test:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/asm-3.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/guice-3.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/javax.inject-1.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/xz-1.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib-examples:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/sources:/csh/link/hadoop/contrib/capacity-scheduler/*.jar:/csh/link/hbase/conf:/csh/link/hive/lib/*&#39; -Djava.library.path=:/csh/software/hadoop-2.7.2/lib/native:/csh/software/hadoop-2.7.2/lib/native org.apache.flume.node.Application --conf-file conf/single-node.conf --name a1&#10;2016-04-19 21:44:11,126 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start(PollingPropertiesFileConfigurationProvider.java:61)] Configuration provider starting&#10;2016-04-19 21:44:11,165 (conf-file-poller-0) [INFO - org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:133)] Reloading configuration file:conf/single-node.conf&#10;2016-04-19 21:44:11,200 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:931)] Added sinks: k1 Agent: a1&#10;2016-04-19 21:44:11,200 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)] Processing:k1&#10;2016-04-19 21:44:11,201 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)] Processing:k1&#10;2016-04-19 21:44:11,285 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration.validateConfiguration(FlumeConfiguration.java:141)] Post-validation flume configuration contains configuration for agents: [a1]&#10;2016-04-19 21:44:11,285 (conf-file-poller-0) [INFO - org.apache.flume.node.AbstractConfigurationProvider.loadChannels(AbstractConfigurationProvider.java:145)] Creating channels&#10;2016-04-19 21:44:11,341 (conf-file-poller-0) [INFO - org.apache.flume.channel.DefaultChannelFactory.create(DefaultChannelFactory.java:42)] Creating instance of channel c1 type memory&#10;2016-04-19 21:44:11,506 (conf-file-poller-0) [INFO - org.apache.flume.node.AbstractConfigurationProvider.loadChannels(AbstractConfigurationProvider.java:200)] Created channel c1&#10;2016-04-19 21:44:11,507 (conf-file-poller-0) [INFO - org.apache.flume.source.DefaultSourceFactory.create(DefaultSourceFactory.java:41)] Creating instance of source r1, type netcat&#10;2016-04-19 21:44:11,590 (conf-file-poller-0) [INFO - org.apache.flume.sink.DefaultSinkFactory.create(DefaultSinkFactory.java:42)] Creating instance of sink: k1, type: logger&#10;2016-04-19 21:44:11,610 (conf-file-poller-0) [INFO - org.apache.flume.node.AbstractConfigurationProvider.getConfiguration(AbstractConfigurationProvider.java:114)] Channel c1 connected to [r1, k1]&#10;2016-04-19 21:44:11,684 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:138)] Starting new configuration:&#123; sourceRunners:&#123;r1=EventDrivenSourceRunner: &#123; source:org.apache.flume.source.NetcatSource&#123;name:r1,state:IDLE&#125; &#125;&#125; sinkRunners:&#123;k1=SinkRunner: &#123; policy:org.apache.flume.sink.DefaultSinkProcessor@7ceacc49 counterGroup:&#123; name:null counters:&#123;&#125; &#125; &#125;&#125; channels:&#123;c1=org.apache.flume.channel.MemoryChannel&#123;name: c1&#125;&#125; &#125;&#10;2016-04-19 21:44:11,713 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:145)] Starting Channel c1&#10;2016-04-19 21:44:17,439 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:120)] Monitored counter group for type: CHANNEL, name: c1: Successfully registered new MBean.&#10;2016-04-19 21:44:17,440 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:96)] Component type: CHANNEL, name: c1 started&#10;2016-04-19 21:44:17,452 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:173)] Starting Sink k1&#10;2016-04-19 21:44:17,462 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:184)] Starting Source r1&#10;2016-04-19 21:44:17,474 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.source.NetcatSource.start(NetcatSource.java:150)] Source starting&#10;2016-04-19 21:44:18,012 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.source.NetcatSource.start(NetcatSource.java:164)] Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444]</span><br></pre></td></tr></table></figure></p>
<h3 id="u6D4B_u8BD5"><a href="#u6D4B_u8BD5" class="headerlink" title="测试"></a>测试</h3><p>在XShell中打开另一个窗口输入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# telnet 127.0.0.1 44444&#10;Trying 127.0.0.1...&#10;Connected to 127.0.0.1.&#10;Escape character is &#39;^]&#39;.&#10;hello flume!&#10;OK&#10;hello big data&#10;OK</span><br></pre></td></tr></table></figure>
<p>在原来启动flume的命令行中可以看到：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2016-04-19 21:47:35,653 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:94)] Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F 20 66 6C 75 6D 65 21 0D          hello flume!. &#125;&#10;2016-04-19 21:49:10,610 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:94)] Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F 20 62 69 67 20 64 61 74 61 0D    hello big data. &#125;</span><br></pre></td></tr></table></figure>
          
        
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/04/21/Sqoop HDFS导入到MySQL/" itemprop="url">
                  Sqoop HDFS导入到MySQL
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-04-21T16:28:01+08:00" content="2016-04-21">
              2016-04-21
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Sqoop/" itemprop="url" rel="index">
                    <span itemprop="name">Sqoop</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/04/21/Sqoop HDFS导入到MySQL/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/04/21/Sqoop HDFS导入到MySQL/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/04/21/Sqoop HDFS导入到MySQL/" class="leancloud_visitors" data-flag-title="Sqoop HDFS导入到MySQL">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="u5728MySQL_u4E2D_u521B_u5EFA_u8868"><a href="#u5728MySQL_u4E2D_u521B_u5EFA_u8868" class="headerlink" title="在MySQL中创建表"></a>在MySQL中创建表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE `tree1` (&#10;  `id` int(11) NOT NULL AUTO_INCREMENT,&#10;  `treeNumber` varchar(100) NOT NULL,&#10;  `productinformationId` int(11) NOT NULL,&#10;  PRIMARY KEY (`id`)&#10;) DEFAULT CHARSET=utf8;</span><br></pre></td></tr></table></figure>
<h3 id="u6267_u884C_u547D_u4EE4"><a href="#u6267_u884C_u547D_u4EE4" class="headerlink" title="执行命令"></a>执行命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop export --connect jdbc:mysql://node1:3306/phx \--username root \--table tree1 -m 1 \--export-dir /sqoop/tree2</span><br></pre></td></tr></table></figure>
<h3 id="u7ED3_u679C"><a href="#u7ED3_u679C" class="headerlink" title="结果"></a>结果</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 conf]# sqoop export --connect jdbc:mysql://node1:3306/phx \--username root \--table tree1 -m 1 \--export-dir /sqoop/tree2&#10;Warning: /csh/link/sqoop/../hcatalog does not exist! HCatalog jobs will fail.&#10;Please set $HCAT_HOME to the root of your HCatalog installation.&#10;Warning: /csh/link/sqoop/../accumulo does not exist! Accumulo imports will fail.&#10;Please set $ACCUMULO_HOME to the root of your Accumulo installation.&#10;16/04/19 01:10:23 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6&#10;16/04/19 01:10:23 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.&#10;16/04/19 01:10:23 INFO tool.CodeGenTool: Beginning code generation&#10;16/04/19 01:10:24 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `tree1` AS t LIMIT 1&#10;16/04/19 01:10:24 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `tree1` AS t LIMIT 1&#10;16/04/19 01:10:24 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /csh/link/hadoop&#10;Note: /tmp/sqoop-root/compile/8efdea8893437a1359c77de6d1bd2395/tree1.java uses or overrides a deprecated API.&#10;Note: Recompile with -Xlint:deprecation for details.&#10;16/04/19 01:10:28 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/8efdea8893437a1359c77de6d1bd2395/tree1.jar&#10;16/04/19 01:10:29 INFO mapreduce.ExportJobBase: Beginning export of tree1&#10;SLF4J: Class path contains multiple SLF4J bindings.&#10;SLF4J: Found binding in [jar:file:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#10;SLF4J: Found binding in [jar:file:/csh/software/hbase-1.1.4/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#10;SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.&#10;SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]&#10;16/04/19 01:10:30 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar&#10;16/04/19 01:10:38 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative&#10;16/04/19 01:10:38 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative&#10;16/04/19 01:10:38 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps&#10;16/04/19 01:10:38 INFO client.RMProxy: Connecting to ResourceManager at node1/192.168.161.11:8032&#10;16/04/19 01:10:48 INFO input.FileInputFormat: Total input paths to process : 1&#10;16/04/19 01:10:48 INFO input.FileInputFormat: Total input paths to process : 1&#10;16/04/19 01:10:49 INFO mapreduce.JobSubmitter: number of splits:1&#10;16/04/19 01:10:49 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative&#10;16/04/19 01:10:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1461026006811_0003&#10;16/04/19 01:10:51 INFO impl.YarnClientImpl: Submitted application application_1461026006811_0003&#10;16/04/19 01:10:51 INFO mapreduce.Job: The url to track the job: http://node1:8088/proxy/application_1461026006811_0003/&#10;16/04/19 01:10:51 INFO mapreduce.Job: Running job: job_1461026006811_0003&#10;16/04/19 01:11:33 INFO mapreduce.Job: Job job_1461026006811_0003 running in uber mode : false&#10;16/04/19 01:11:33 INFO mapreduce.Job:  map 0% reduce 0%&#10;16/04/19 01:12:24 INFO mapreduce.Job:  map 100% reduce 0%&#10;16/04/19 01:12:27 INFO mapreduce.Job: Job job_1461026006811_0003 completed successfully&#10;16/04/19 01:12:27 INFO mapreduce.Job: Counters: 30&#10;&#9;File System Counters&#10;&#9;&#9;FILE: Number of bytes read=0&#10;&#9;&#9;FILE: Number of bytes written=137080&#10;&#9;&#9;FILE: Number of read operations=0&#10;&#9;&#9;FILE: Number of large read operations=0&#10;&#9;&#9;FILE: Number of write operations=0&#10;&#9;&#9;HDFS: Number of bytes read=251&#10;&#9;&#9;HDFS: Number of bytes written=0&#10;&#9;&#9;HDFS: Number of read operations=4&#10;&#9;&#9;HDFS: Number of large read operations=0&#10;&#9;&#9;HDFS: Number of write operations=0&#10;&#9;Job Counters &#10;&#9;&#9;Launched map tasks=1&#10;&#9;&#9;Rack-local map tasks=1&#10;&#9;&#9;Total time spent by all maps in occupied slots (ms)=45106&#10;&#9;&#9;Total time spent by all reduces in occupied slots (ms)=0&#10;&#9;&#9;Total time spent by all map tasks (ms)=45106&#10;&#9;&#9;Total vcore-milliseconds taken by all map tasks=45106&#10;&#9;&#9;Total megabyte-milliseconds taken by all map tasks=46188544&#10;&#9;Map-Reduce Framework&#10;&#9;&#9;Map input records=7&#10;&#9;&#9;Map output records=7&#10;&#9;&#9;Input split bytes=122&#10;&#9;&#9;Spilled Records=0&#10;&#9;&#9;Failed Shuffles=0&#10;&#9;&#9;Merged Map outputs=0&#10;&#9;&#9;GC time elapsed (ms)=258&#10;&#9;&#9;CPU time spent (ms)=1680&#10;&#9;&#9;Physical memory (bytes) snapshot=86806528&#10;&#9;&#9;Virtual memory (bytes) snapshot=2086785024&#10;&#9;&#9;Total committed heap usage (bytes)=17235968&#10;&#9;File Input Format Counters &#10;&#9;&#9;Bytes Read=0&#10;&#9;File Output Format Counters &#10;&#9;&#9;Bytes Written=0&#10;16/04/19 01:12:27 INFO mapreduce.ExportJobBase: Transferred 251 bytes in 109.7035 seconds (2.288 bytes/sec)&#10;16/04/19 01:12:27 INFO mapreduce.ExportJobBase: Exported 7 records.</span><br></pre></td></tr></table></figure>
<h3 id="u5728MySQL_u4E2D_u67E5_u770Btree1_u8868_u4E2D_u6570_u636E"><a href="#u5728MySQL_u4E2D_u67E5_u770Btree1_u8868_u4E2D_u6570_u636E" class="headerlink" title="在MySQL中查看tree1表中数据"></a>在MySQL中查看tree1表中数据</h3><p><img src="https://1csh1.github.io/img/Sqoop%20HDFS导入到MySQL/数据库表.jpg" alt="数据库表"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/04/21/Sqoop MySQL 导入到Hive/" itemprop="url">
                  Sqoop MySQL 导入到Hive
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-04-21T15:52:20+08:00" content="2016-04-21">
              2016-04-21
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Sqoop/" itemprop="url" rel="index">
                    <span itemprop="name">Sqoop</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/04/21/Sqoop MySQL 导入到Hive/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/04/21/Sqoop MySQL 导入到Hive/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/04/21/Sqoop MySQL 导入到Hive/" class="leancloud_visitors" data-flag-title="Sqoop MySQL 导入到Hive">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="u5C06_u6570_u636E_u5E93phx_u4E2D_u7684tree_u8868_u7684_u6570_u636E_u5BFC_u5165_u5230Hive_u4E2D"><a href="#u5C06_u6570_u636E_u5E93phx_u4E2D_u7684tree_u8868_u7684_u6570_u636E_u5BFC_u5165_u5230Hive_u4E2D" class="headerlink" title="将数据库phx中的tree表的数据导入到Hive中"></a>将数据库phx中的tree表的数据导入到Hive中</h3><p>命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop import --connect jdbc:mysql://node1:3306/phx \--username root \--table tree \--hive-import \--hive-overwrite \--create-hive-table \--hive-table tree1 \--target-dir /sqoop/tree3</span><br></pre></td></tr></table></figure>
<p>结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 conf]# sqoop import --connect jdbc:mysql://node1:3306/phx \--username root \--table tree \--hive-import \--hive-overwrite \--create-hive-table \--hive-table tree1 \--target-dir /sqoop/tree3&#10;Warning: /csh/link/sqoop/../hcatalog does not exist! HCatalog jobs will fail.&#10;Please set $HCAT_HOME to the root of your HCatalog installation.&#10;Warning: /csh/link/sqoop/../accumulo does not exist! Accumulo imports will fail.&#10;Please set $ACCUMULO_HOME to the root of your Accumulo installation.&#10;16/04/18 21:20:00 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6&#10;16/04/18 21:20:00 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override&#10;16/04/18 21:20:00 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.&#10;16/04/18 21:20:00 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.&#10;16/04/18 21:20:00 INFO tool.CodeGenTool: Beginning code generation&#10;16/04/18 21:20:00 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `tree` AS t LIMIT 1&#10;16/04/18 21:20:00 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `tree` AS t LIMIT 1&#10;16/04/18 21:20:00 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /csh/link/hadoop&#10;Note: /tmp/sqoop-root/compile/9cff004a8ff405b712c978864c2775df/tree.java uses or overrides a deprecated API.&#10;Note: Recompile with -Xlint:deprecation for details.&#10;16/04/18 21:20:03 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/9cff004a8ff405b712c978864c2775df/tree.jar&#10;16/04/18 21:20:03 WARN manager.MySQLManager: It looks like you are importing from mysql.&#10;16/04/18 21:20:03 WARN manager.MySQLManager: This transfer can be faster! Use the --direct&#10;16/04/18 21:20:03 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.&#10;16/04/18 21:20:03 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)&#10;16/04/18 21:20:03 INFO mapreduce.ImportJobBase: Beginning import of tree&#10;SLF4J: Class path contains multiple SLF4J bindings.&#10;SLF4J: Found binding in [jar:file:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#10;SLF4J: Found binding in [jar:file:/csh/software/hbase-1.1.4/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#10;SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.&#10;SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]&#10;16/04/18 21:20:04 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar&#10;16/04/18 21:20:09 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps&#10;16/04/18 21:20:10 INFO client.RMProxy: Connecting to ResourceManager at node1/192.168.161.11:8032&#10;16/04/18 21:20:23 INFO db.DBInputFormat: Using read commited transaction isolation&#10;16/04/18 21:20:23 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `tree`&#10;16/04/18 21:20:24 INFO mapreduce.JobSubmitter: number of splits:4&#10;16/04/18 21:20:25 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1461026006811_0001&#10;16/04/18 21:20:27 INFO impl.YarnClientImpl: Submitted application application_1461026006811_0001&#10;16/04/18 21:20:28 INFO mapreduce.Job: The url to track the job: http://node1:8088/proxy/application_1461026006811_0001/&#10;16/04/18 21:20:28 INFO mapreduce.Job: Running job: job_1461026006811_0001&#10;16/04/18 21:21:01 INFO mapreduce.Job: Job job_1461026006811_0001 running in uber mode : false&#10;16/04/18 21:21:01 INFO mapreduce.Job:  map 0% reduce 0%&#10;16/04/18 21:22:00 INFO mapreduce.Job:  map 25% reduce 0%&#10;16/04/18 21:23:13 INFO mapreduce.Job:  map 50% reduce 0%&#10;16/04/18 21:23:14 INFO mapreduce.Job:  map 100% reduce 0%&#10;16/04/18 21:23:21 INFO mapreduce.Job: Job job_1461026006811_0001 completed successfully&#10;16/04/18 21:23:22 INFO mapreduce.Job: Counters: 31&#10;&#9;File System Counters&#10;&#9;&#9;FILE: Number of bytes read=0&#10;&#9;&#9;FILE: Number of bytes written=549568&#10;&#9;&#9;FILE: Number of read operations=0&#10;&#9;&#9;FILE: Number of large read operations=0&#10;&#9;&#9;FILE: Number of write operations=0&#10;&#9;&#9;HDFS: Number of bytes read=398&#10;&#9;&#9;HDFS: Number of bytes written=373&#10;&#9;&#9;HDFS: Number of read operations=16&#10;&#9;&#9;HDFS: Number of large read operations=0&#10;&#9;&#9;HDFS: Number of write operations=8&#10;&#9;Job Counters &#10;&#9;&#9;Killed map tasks=4&#10;&#9;&#9;Launched map tasks=7&#10;&#9;&#9;Other local map tasks=7&#10;&#9;&#9;Total time spent by all maps in occupied slots (ms)=585524&#10;&#9;&#9;Total time spent by all reduces in occupied slots (ms)=0&#10;&#9;&#9;Total time spent by all map tasks (ms)=585524&#10;&#9;&#9;Total vcore-milliseconds taken by all map tasks=585524&#10;&#9;&#9;Total megabyte-milliseconds taken by all map tasks=599576576&#10;&#9;Map-Reduce Framework&#10;&#9;&#9;Map input records=21&#10;&#9;&#9;Map output records=21&#10;&#9;&#9;Input split bytes=398&#10;&#9;&#9;Spilled Records=0&#10;&#9;&#9;Failed Shuffles=0&#10;&#9;&#9;Merged Map outputs=0&#10;&#9;&#9;GC time elapsed (ms)=722&#10;&#9;&#9;CPU time spent (ms)=15670&#10;&#9;&#9;Physical memory (bytes) snapshot=394702848&#10;&#9;&#9;Virtual memory (bytes) snapshot=8350515200&#10;&#9;&#9;Total committed heap usage (bytes)=68792320&#10;&#9;File Input Format Counters &#10;&#9;&#9;Bytes Read=0&#10;&#9;File Output Format Counters &#10;&#9;&#9;Bytes Written=373&#10;16/04/18 21:23:22 INFO mapreduce.ImportJobBase: Transferred 373 bytes in 193.0206 seconds (1.9324 bytes/sec)&#10;16/04/18 21:23:22 INFO mapreduce.ImportJobBase: Retrieved 21 records.&#10;16/04/18 21:23:23 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `tree` AS t LIMIT 1&#10;16/04/18 21:23:23 INFO hive.HiveImport: Loading uploaded data into Hive&#10;16/04/18 21:25:54 INFO hive.HiveImport: &#10;16/04/18 21:25:54 INFO hive.HiveImport: Logging initialized using configuration in file:/csh/software/apache-hive-1.2.1-bin/conf/hive-log4j.properties&#10;16/04/18 21:25:57 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.&#10;16/04/18 21:25:57 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#10;16/04/18 21:25:57 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/csh/software/hbase-1.1.4/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#10;16/04/18 21:25:57 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.&#10;16/04/18 21:25:57 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]&#10;16/04/18 21:27:27 INFO hive.HiveImport: OK&#10;16/04/18 21:27:28 INFO hive.HiveImport: Time taken: 23.5 seconds&#10;16/04/18 21:27:30 INFO hive.HiveImport: Loading data to table default.tree1&#10;16/04/18 21:27:35 INFO hive.HiveImport: Table default.tree1 stats: [numFiles=4, numRows=0, totalSize=373, rawDataSize=0]&#10;16/04/18 21:27:35 INFO hive.HiveImport: OK&#10;16/04/18 21:27:35 INFO hive.HiveImport: Time taken: 8.27 seconds&#10;16/04/18 21:27:36 INFO hive.HiveImport: Hive import complete.&#10;16/04/18 21:27:40 INFO hive.HiveImport: Export directory is contains the _SUCCESS file only, removing the directory.</span><br></pre></td></tr></table></figure>
<p>浏览HDFS如下图：<br><img src="https://1csh1.github.io/img/Sqoop%20MySQL%20导入到Hive/HDFS.jpg" alt="HDFS"></p>
<p>在Hive中检查是否有数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&#62; select * from tree1;&#10;OK&#10;1&#9;466464684640&#9;1&#10;2&#9;466464684641&#9;2&#10;3&#9;466464684642&#9;3&#10;4&#9;466464684643&#9;1&#10;5&#9;466464684644&#9;2&#10;6&#9;466464684645&#9;3&#10;7&#9;466464684646&#9;1&#10;8&#9;466464684647&#9;2&#10;9&#9;466464684648&#9;3&#10;10&#9;466464684649&#9;1&#10;11&#9;4664646846410&#9;2&#10;12&#9;4664646846411&#9;3&#10;13&#9;4664646846412&#9;1&#10;14&#9;4664646846413&#9;2&#10;15&#9;4664646846414&#9;3&#10;16&#9;4664646846415&#9;1&#10;17&#9;4664646846416&#9;2&#10;18&#9;4664646846417&#9;3&#10;19&#9;4664646846418&#9;1&#10;20&#9;4664646846419&#9;2&#10;21&#9;111111&#9;1&#10;Time taken: 2.622 seconds, Fetched: 21 row(s)</span><br></pre></td></tr></table></figure>
<h3 id="u53C2_u6570_u8BF4_u660E"><a href="#u53C2_u6570_u8BF4_u660E" class="headerlink" title="参数说明"></a>参数说明</h3><table>
<thead>
<tr>
<th style="text-align:left">参数</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">–hive-home <dir></dir></td>
<td style="text-align:left">Hive的安装目录，可以通过该参数覆盖掉默认的hive目录</td>
</tr>
<tr>
<td style="text-align:left">–hive-overwrite</td>
<td style="text-align:left">覆盖掉在hive表中已经存在的数据</td>
</tr>
<tr>
<td style="text-align:left">–create-hive-table</td>
<td style="text-align:left">默认是false，如果目标表已经存在了，那么创建任务会失败</td>
</tr>
<tr>
<td style="text-align:left">–hive-table</td>
<td style="text-align:left">后面接要创建的hive表</td>
</tr>
<tr>
<td style="text-align:left">–table</td>
<td style="text-align:left">指定关系数据库表名</td>
</tr>
</tbody>
</table>

          
        
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/default_avatar.jpg"
               alt="James-CSH" />
          <p class="site-author-name" itemprop="name">James-CSH</p>
          <p class="site-description motion-element" itemprop="description">当才华撑不起野心时，应该静下心来学习；当能力驾驭不了目标时，应该沉下心来历练。</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">46</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">25</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">40</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        <div class="links-of-blogroll motion-element">
          
        </div>

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script type="text/javascript" src="http://tajs.qq.com/stats?sId=55186219" charset="UTF-8"></script>
<div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">James-CSH</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

<div>
<span id="busuanzi_container_site_uv">
  本站访客数<span id="busuanzi_value_site_uv"></span>人次
</span>
&nbsp;|&nbsp;
<span id="busuanzi_container_site_pv">
    本站总访问量<span id="busuanzi_value_site_pv"></span>次
</span>  
</div>



      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  


  




<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>

  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=0.5.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=0.5.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=0.5.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"1CSH1"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
  





  
  
  

  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("eodGFnKXWTUNhQKhMktCLDNj-gzGzoHsz", "iC7dXG3oqjDHhiy0K3MmaIlE");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



</body>
</html>
