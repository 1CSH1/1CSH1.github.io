<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"/>




  <link href="//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">



<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=0.5.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=0.5.0" />






<meta name="description" content="当才华撑不起野心时，应该静下心来学习；当能力驾驭不了目标时，应该沉下心来历练。">
<meta property="og:type" content="website">
<meta property="og:title" content="LieBrother">
<meta property="og:url" content="https://1csh1.github.io/page/20/index.html">
<meta property="og:site_name" content="LieBrother">
<meta property="og:description" content="当才华撑不起野心时，应该静下心来学习；当能力驾驭不了目标时，应该沉下心来历练。">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LieBrother">
<meta name="twitter:description" content="当才华撑不起野心时，应该静下心来学习；当能力驾驭不了目标时，应该沉下心来历练。">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>

  <title> LieBrother </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  





  <script type="text/javascript">
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=55186219";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>






  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">LieBrother</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">当才华撑不起野心时，应该静下心来学习；当能力驾驭不了目标时，应该沉下心来历练。</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home fa-fw"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-archive fa-fw"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-th fa-fw"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="https://mp.weixin.qq.com/s/SSbbNHha4-VDHnKgzYdQRw" rel="section">
            
              <i class="menu-item-icon fa fa-user fa-fw"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-tags fa-fw"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/04/21/Flume集群搭建/" itemprop="url">
                  Flume集群搭建
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-04-21T18:20:43+08:00" content="2016-04-21">
              2016-04-21
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Flume/" itemprop="url" rel="index">
                    <span itemprop="name">Flume</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/04/21/Flume集群搭建/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/04/21/Flume集群搭建/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/04/21/Flume集群搭建/" class="leancloud_visitors" data-flag-title="Flume集群搭建">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>集群的意思是多台机器，最少有2台机器，一台机器从数据源中获取数据，将数据传送到另一台机器上，然后输出。接下来就要实现Flume集群搭建。文中的集群如下图所示。<br><img src="https://1csh1.github.io/img/Flume集群搭建/架构.jpg" alt="架构"><br>这里我们需要2台机器，node1作为push推送数据，node2作为pull获取数据后显示出来。</p>
<h3 id="配置pull-conf"><a href="#配置pull-conf" class="headerlink" title="配置pull.conf"></a>配置pull.conf</h3><p>【在node2机器上操作】<br>在conf目录下创建pull.conf文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">touch pull.conf</span><br></pre></td></tr></table></figure></p>
<p>编辑pull.conf<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#汇总数据代理的配置文件pull.conf</span><br><span class="line">#Name the components on this agent</span><br><span class="line">a1.sources= r1</span><br><span class="line">a1.sinks= k1</span><br><span class="line">a1.channels= c1</span><br><span class="line"> </span><br><span class="line">#Describe/configure the source</span><br><span class="line">a1.sources.r1.type= avro</span><br><span class="line">a1.sources.r1.channels= c1</span><br><span class="line">a1.sources.r1.bind= node2</span><br><span class="line">a1.sources.r1.port= 44444</span><br><span class="line"> </span><br><span class="line">#Describe the sink</span><br><span class="line">a1.sinks.k1.type= logger</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"> </span><br><span class="line">#Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type= memory</span><br><span class="line">a1.channels.c1.keep-alive= 10</span><br><span class="line">a1.channels.c1.capacity= 100000</span><br><span class="line">a1.channels.c1.transactionCapacity= 100000</span><br></pre></td></tr></table></figure></p>
<h3 id="配置push-conf"><a href="#配置push-conf" class="headerlink" title="配置push.conf"></a>配置push.conf</h3><p>【在node1机器上操作】<br>在conf目录下创建push.conf文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">touch push.conf</span><br></pre></td></tr></table></figure></p>
<p>编辑push.conf<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#推数据代理的配置文件push.conf</span><br><span class="line">#Name the components on this agent</span><br><span class="line">a2.sources= r1</span><br><span class="line">a2.sinks= k1</span><br><span class="line">a2.channels= c1</span><br><span class="line"> </span><br><span class="line">#Describe/configure the source</span><br><span class="line">a2.sources.r1.type= spooldir</span><br><span class="line">a2.sources.r1.spoolDir= /csh/hadoop/flume/logs</span><br><span class="line">a2.sources.r1.channels= c1</span><br><span class="line"> </span><br><span class="line">#Use a channel which buffers events in memory</span><br><span class="line">a2.channels.c1.type= memory</span><br><span class="line">a2.channels.c1.keep-alive= 10</span><br><span class="line">a2.channels.c1.capacity= 100000</span><br><span class="line">a2.channels.c1.transactionCapacity= 100000</span><br><span class="line"> </span><br><span class="line">#Describe/configure the source</span><br><span class="line">a2.sinks.k1.type= avro</span><br><span class="line">a2.sinks.k1.channel= c1</span><br><span class="line">a2.sinks.k1.hostname= node2</span><br><span class="line">a2.sinks.k1.port= 44444</span><br></pre></td></tr></table></figure></p>
<h3 id="创建spoolDir目录"><a href="#创建spoolDir目录" class="headerlink" title="创建spoolDir目录"></a>创建spoolDir目录</h3><p>【在node1中进行该操作】<br>根据push.conf中的配置 a2.sources.r1.spoolDir参数，创建目录，如果不先创建目录，则启动时会报错<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /csh/hadoop/flume/logs</span><br></pre></td></tr></table></figure></p>
<h3 id="启动作为pull的主机"><a href="#启动作为pull的主机" class="headerlink" title="启动作为pull的主机"></a>启动作为pull的主机</h3><p>【本文为node2主机】<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node2 flume]# flume-ng agent -c conf -f conf/pull.conf -n a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></p>
<p>显示如下信息则为启动成功</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">2016-04-20 00:08:15,550 (conf-file-poller-0) [INFO - org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:133)] Reloading configuration file:conf/pull.conf</span><br><span class="line">2016-04-20 00:08:15,573 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:931)] Added sinks: k1 Agent: a1</span><br><span class="line">2016-04-20 00:08:15,573 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)] Processing:k1</span><br><span class="line">2016-04-20 00:08:15,574 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)] Processing:k1</span><br><span class="line">2016-04-20 00:08:15,621 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration.validateConfiguration(FlumeConfiguration.java:141)] Post-validation flume configuration contains configuration for agents: [a1]</span><br><span class="line">2016-04-20 00:08:15,622 (conf-file-poller-0) [INFO - org.apache.flume.node.AbstractConfigurationProvider.loadChannels(AbstractConfigurationProvider.java:145)] Creating channels</span><br><span class="line">2016-04-20 00:08:15,658 (conf-file-poller-0) [INFO - org.apache.flume.channel.DefaultChannelFactory.create(DefaultChannelFactory.java:42)] Creating instance of channel c1 type memory</span><br><span class="line">2016-04-20 00:08:15,672 (conf-file-poller-0) [INFO - org.apache.flume.node.AbstractConfigurationProvider.loadChannels(AbstractConfigurationProvider.java:200)] Created channel c1</span><br><span class="line">2016-04-20 00:08:15,677 (conf-file-poller-0) [INFO - org.apache.flume.source.DefaultSourceFactory.create(DefaultSourceFactory.java:41)] Creating instance of source r1, type avro</span><br><span class="line">2016-04-20 00:08:15,732 (conf-file-poller-0) [INFO - org.apache.flume.sink.DefaultSinkFactory.create(DefaultSinkFactory.java:42)] Creating instance of sink: k1, type: logger</span><br><span class="line">2016-04-20 00:08:15,735 (conf-file-poller-0) [INFO - org.apache.flume.node.AbstractConfigurationProvider.getConfiguration(AbstractConfigurationProvider.java:114)] Channel c1 connected to [r1, k1]</span><br><span class="line">2016-04-20 00:08:15,750 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:138)] Starting new configuration:&#123; sourceRunners:&#123;r1=EventDrivenSourceRunner: &#123; source:Avro source r1: &#123; bindAddress: node2, port: 44444 &#125; &#125;&#125; sinkRunners:&#123;k1=SinkRunner: &#123; policy:org.apache.flume.sink.DefaultSinkProcessor@ea5ba80 counterGroup:&#123; name:null counters:&#123;&#125; &#125; &#125;&#125; channels:&#123;c1=org.apache.flume.channel.MemoryChannel&#123;name: c1&#125;&#125; &#125;</span><br><span class="line">2016-04-20 00:08:15,782 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:145)] Starting Channel c1</span><br><span class="line">2016-04-20 00:08:15,784 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:160)] Waiting for channel: c1 to start. Sleeping for 500 ms</span><br><span class="line">2016-04-20 00:08:15,897 (lifecycleSupervisor-1-2) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:120)] Monitored counter group for type: CHANNEL, name: c1: Successfully registered new MBean.</span><br><span class="line">2016-04-20 00:08:15,901 (lifecycleSupervisor-1-2) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:96)] Component type: CHANNEL, name: c1 started</span><br><span class="line">2016-04-20 00:08:16,285 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:173)] Starting Sink k1</span><br><span class="line">2016-04-20 00:08:16,288 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:184)] Starting Source r1</span><br><span class="line">2016-04-20 00:08:16,298 (lifecycleSupervisor-1-2) [INFO - org.apache.flume.source.AvroSource.start(AvroSource.java:228)] Starting Avro source r1: &#123; bindAddress: node2, port: 44444 &#125;...</span><br><span class="line">2016-04-20 00:08:16,951 (lifecycleSupervisor-1-2) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:120)] Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.</span><br><span class="line">2016-04-20 00:08:16,952 (lifecycleSupervisor-1-2) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:96)] Component type: SOURCE, name: r1 started</span><br><span class="line">2016-04-20 00:08:16,959 (lifecycleSupervisor-1-2) [INFO - org.apache.flume.source.AvroSource.start(AvroSource.java:253)] Avro source r1 started.</span><br></pre></td></tr></table></figure>
<h3 id="启动作为push的主机"><a href="#启动作为push的主机" class="headerlink" title="启动作为push的主机"></a>启动作为push的主机</h3><p>【本文为node1主机】<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 flume]# flume-ng agent -n a2 -c conf -f conf/push.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></p>
<p>显示如下信息则为启动成功<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">2016-04-20 00:11:58,196 (conf-file-poller-0) [INFO - org.apache.flume.node.AbstractConfigurationProvider.getConfiguration(AbstractConfigurationProvider.java:114)] Channel c1 connected to [r1, k1]</span><br><span class="line">2016-04-20 00:11:58,226 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:138)] Starting new configuration:&#123; sourceRunners:&#123;r1=EventDrivenSourceRunner: &#123; source:Spool Directory source r1: &#123; spoolDir: /csh/hadoop/flume/logs &#125; &#125;&#125; sinkRunners:&#123;k1=SinkRunner: &#123; policy:org.apache.flume.sink.DefaultSinkProcessor@6b089e25 counterGroup:&#123; name:null counters:&#123;&#125; &#125; &#125;&#125; channels:&#123;c1=org.apache.flume.channel.MemoryChannel&#123;name: c1&#125;&#125; &#125;</span><br><span class="line">2016-04-20 00:11:58,236 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:145)] Starting Channel c1</span><br><span class="line">2016-04-20 00:11:58,360 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:120)] Monitored counter group for type: CHANNEL, name: c1: Successfully registered new MBean.</span><br><span class="line">2016-04-20 00:11:58,361 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:96)] Component type: CHANNEL, name: c1 started</span><br><span class="line">2016-04-20 00:11:58,362 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:173)] Starting Sink k1</span><br><span class="line">2016-04-20 00:11:58,369 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:184)] Starting Source r1</span><br><span class="line">2016-04-20 00:11:58,372 (lifecycleSupervisor-1-4) [INFO - org.apache.flume.source.SpoolDirectorySource.start(SpoolDirectorySource.java:78)] SpoolDirectorySource source starting with directory: /csh/hadoop/flume/logs</span><br><span class="line">2016-04-20 00:11:58,388 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.sink.AbstractRpcSink.start(AbstractRpcSink.java:289)] Starting RpcSink k1 &#123; host: node2, port: 44444 &#125;...</span><br><span class="line">2016-04-20 00:11:58,409 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:120)] Monitored counter group for type: SINK, name: k1: Successfully registered new MBean.</span><br><span class="line">2016-04-20 00:11:58,409 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:96)] Component type: SINK, name: k1 started</span><br><span class="line">2016-04-20 00:11:58,409 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.sink.AbstractRpcSink.createConnection(AbstractRpcSink.java:206)] Rpc sink k1: Building RpcClient with hostname: node2, port: 44444</span><br><span class="line">2016-04-20 00:11:58,410 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.sink.AvroSink.initializeRpcClient(AvroSink.java:126)] Attempting to create Avro Rpc client.</span><br><span class="line">2016-04-20 00:11:58,458 (lifecycleSupervisor-1-0) [WARN - org.apache.flume.api.NettyAvroRpcClient.configure(NettyAvroRpcClient.java:634)] Using default maxIOWorkers</span><br><span class="line">2016-04-20 00:11:58,536 (lifecycleSupervisor-1-4) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:120)] Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.</span><br><span class="line">2016-04-20 00:11:58,536 (lifecycleSupervisor-1-4) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:96)] Component type: SOURCE, name: r1 started</span><br><span class="line">2016-04-20 00:11:59,263 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.sink.AbstractRpcSink.start(AbstractRpcSink.java:303)] Rpc sink k1 started.</span><br></pre></td></tr></table></figure></p>
<p>这时pull主机【本文为node2】输出信息表示连接成功<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2016-04-20 00:11:58,875 (New I/O server boss #1 ([id: 0x71ba9ce2, /192.168.161.12:44444])) [INFO - org.apache.avro.ipc.NettyServer$NettyServerAvroHandler.handleUpstream(NettyServer.java:171)] [id: 0x7d9299a9, /192.168.161.11:44003 =&gt; /192.168.161.12:44444] OPEN</span><br><span class="line">2016-04-20 00:11:58,880 (New I/O  worker #1) [INFO - org.apache.avro.ipc.NettyServer$NettyServerAvroHandler.handleUpstream(NettyServer.java:171)] [id: 0x7d9299a9, /192.168.161.11:44003 =&gt; /192.168.161.12:44444] BOUND: /192.168.161.12:44444</span><br><span class="line">2016-04-20 00:11:58,884 (New I/O  worker #1) [INFO - org.apache.avro.ipc.NettyServer$NettyServerAvroHandler.handleUpstream(NettyServer.java:171)] [id: 0x7d9299a9, /192.168.161.11:44003 =&gt; /192.168.161.12:44444] CONNECTED: /192.168.161.11:44003</span><br></pre></td></tr></table></figure></p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>在push主机中【本文为node1】的spoolDir目录【本文为/csh/hadoop/flume/logs】中创建test.log<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vi test.log</span><br><span class="line"># 输入内容 hello flume</span><br></pre></td></tr></table></figure></p>
<p>这时push主机【本文为node1】中命令行输出如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2016-04-20 00:13:09,274 (pool-4-thread-1) [INFO - org.apache.flume.client.avro.ReliableSpoolingFileEventReader.readEvents(ReliableSpoolingFileEventReader.java:258)] Last read took us just up to a file boundary. Rolling to the next file, if there is one.</span><br><span class="line">2016-04-20 00:13:09,275 (pool-4-thread-1) [INFO - org.apache.flume.client.avro.ReliableSpoolingFileEventReader.rollCurrentFile(ReliableSpoolingFileEventReader.java:348)] Preparing to move file /csh/hadoop/flume/logs/test.log to /csh/hadoop/flume/logs/test.log.COMPLETED</span><br></pre></td></tr></table></figure></p>
<p>pull主机【本文为node2】中命令行输出如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2016-04-20 00:13:21,344 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:94)] Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F 20 66 6C 75 6D 65                hello flume &#125;</span><br></pre></td></tr></table></figure></p>
<p>证明Flume集群搭建成功</p>
<p>我们可以发现test.log被改名为test.log.COMPLETED</p>

          
        
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/04/21/Flume单机安装与配置/" itemprop="url">
                  Flume单机安装与配置
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-04-21T18:15:35+08:00" content="2016-04-21">
              2016-04-21
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Flume/" itemprop="url" rel="index">
                    <span itemprop="name">Flume</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/04/21/Flume单机安装与配置/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/04/21/Flume单机安装与配置/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/04/21/Flume单机安装与配置/" class="leancloud_visitors" data-flag-title="Flume单机安装与配置">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><p><a href="https://1csh1.github.io/file/Flume单机安装与配置/apache-flume-1.6.0-bin.tar.gz">apache-flume-1.6.0-bin.tar.gz</a></p>
<h3 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-flume-1.6.0-bin.tar.gz</span><br></pre></td></tr></table></figure>
<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp conf/flume-conf.properties.template conf/flume-conf.properties</span><br><span class="line">cp conf/flume-env.sh.template conf/flume-env.sh</span><br></pre></td></tr></table></figure>
<p>flume-env.sh<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Enviroment variables can be set here.</span><br><span class="line">export JAVA_HOME=/csh/link/jdk</span><br><span class="line"></span><br><span class="line"># Give Flume more memory and pre-allocate, enable remote monitoring via JMX</span><br><span class="line"># export JAVA_OPTS=&quot;-Xms100m -Xmx2000m -Dcom.sun.management.jmxremote&quot;</span><br><span class="line"></span><br><span class="line"># Note that the Flume conf directory is always included in the classpath.</span><br><span class="line">FLUME_CLASSPATH=/csh/link/flume/lib</span><br></pre></td></tr></table></figure></p>
<h3 id="单机配置"><a href="#单机配置" class="headerlink" title="单机配置"></a>单机配置</h3><p>在conf目录下创建single-node.conf文件，并将下面内容复制粘贴</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># example.conf: A single-node Flume configuration</span><br><span class="line"></span><br><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
<h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent --conf conf --conf-file conf/single-node.conf --name a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>
<p>命令行输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">Info: Sourcing environment configuration script /csh/link/flume/conf/flume-env.sh</span><br><span class="line">Info: Including Hadoop libraries found via (/csh/link/hadoop/bin/hadoop) for HDFS access</span><br><span class="line">Info: Excluding /csh/software/hadoop-2.7.2/share/hadoop/common/lib/slf4j-api-1.7.10.jar from classpath</span><br><span class="line">Info: Excluding /csh/software/hadoop-2.7.2/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar from classpath</span><br><span class="line">Info: Including HBASE libraries found via (/csh/link/hbase/bin/hbase) for HBASE access</span><br><span class="line">Info: Excluding /csh/link/hbase/lib/slf4j-api-1.7.7.jar from classpath</span><br><span class="line">Info: Excluding /csh/link/hbase/lib/slf4j-log4j12-1.7.5.jar from classpath</span><br><span class="line">Info: Excluding /csh/software/hadoop-2.7.2/share/hadoop/common/lib/slf4j-api-1.7.10.jar from classpath</span><br><span class="line">Info: Excluding /csh/software/hadoop-2.7.2/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar from classpath</span><br><span class="line">Info: Including Hive libraries found via (/csh/link/hive) for Hive access</span><br><span class="line">+ exec /csh/link/jdk/bin/java -Xmx20m -Dflume.root.logger=INFO,console -cp &apos;/csh/link/flume/conf:/csh/link/flume/lib/*:/csh/link/flume/lib:/csh/software/hadoop-2.7.2/etc/hadoop:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/activation-1.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/asm-3.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/avro-1.7.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-cli-1.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-codec-1.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-collections-3.2.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-compress-1.4.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-configuration-1.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-digester-1.8.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-httpclient-3.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-io-2.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-lang-2.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-net-3.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/curator-client-2.7.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/curator-framework-2.7.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/gson-2.2.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/guava-11.0.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/hadoop-annotations-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/hadoop-auth-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/hamcrest-core-1.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/httpclient-4.2.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/httpcore-4.2.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jersey-core-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jersey-json-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jersey-server-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jets3t-0.9.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jettison-1.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jetty-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jetty-util-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jsch-0.1.42.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jsp-api-2.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jsr305-3.0.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/junit-4.11.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/log4j-1.2.17.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/mockito-all-1.8.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/netty-3.6.2.Final.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/paranamer-2.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/servlet-api-2.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/stax-api-1.0-2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/xmlenc-0.52.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/xz-1.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/zookeeper-3.4.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/hadoop-common-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/hadoop-common-2.7.2-tests.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/hadoop-nfs-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/jdiff:/csh/software/hadoop-2.7.2/share/hadoop/common/lib:/csh/software/hadoop-2.7.2/share/hadoop/common/sources:/csh/software/hadoop-2.7.2/share/hadoop/common/templates:/csh/software/hadoop-2.7.2/share/hadoop/hdfs:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/asm-3.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-io-2.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/guava-11.0.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/hadoop-hdfs-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/hadoop-hdfs-2.7.2-tests.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/jdiff:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/sources:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/templates:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/webapps:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/activation-1.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/asm-3.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-cli-1.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-codec-1.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-io-2.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/guava-11.0.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/guice-3.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/javax.inject-1.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jersey-client-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jersey-core-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jersey-json-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jersey-server-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jettison-1.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jetty-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/log4j-1.2.17.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/servlet-api-2.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/xz-1.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-api-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-client-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-common-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-registry-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-common-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib:/csh/software/hadoop-2.7.2/share/hadoop/yarn/sources:/csh/software/hadoop-2.7.2/share/hadoop/yarn/test:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/asm-3.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/guice-3.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/javax.inject-1.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/xz-1.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib-examples:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/sources:/csh/link/hadoop/contrib/capacity-scheduler/*.jar:/csh/link/hbase/conf:/csh/link/jdk/lib/tools.jar:/csh/link/hbase:/csh/link/hbase/lib/activation-1.1.jar:/csh/link/hbase/lib/antisamy-1.4.3.jar:/csh/link/hbase/lib/aopalliance-1.0.jar:/csh/link/hbase/lib/apacheds-i18n-2.0.0-M15.jar:/csh/link/hbase/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/csh/link/hbase/lib/api-asn1-api-1.0.0-M20.jar:/csh/link/hbase/lib/api-util-1.0.0-M20.jar:/csh/link/hbase/lib/asm-3.1.jar:/csh/link/hbase/lib/avro-1.7.4.jar:/csh/link/hbase/lib/batik-css-1.7.jar:/csh/link/hbase/lib/batik-ext-1.7.jar:/csh/link/hbase/lib/batik-util-1.7.jar:/csh/link/hbase/lib/bsh-core-2.0b4.jar:/csh/link/hbase/lib/commons-beanutils-1.7.0.jar:/csh/link/hbase/lib/commons-beanutils-core-1.7.0.jar:/csh/link/hbase/lib/commons-cli-1.2.jar:/csh/link/hbase/lib/commons-codec-1.9.jar:/csh/link/hbase/lib/commons-collections-3.2.2.jar:/csh/link/hbase/lib/commons-compress-1.4.1.jar:/csh/link/hbase/lib/commons-configuration-1.6.jar:/csh/link/hbase/lib/commons-daemon-1.0.13.jar:/csh/link/hbase/lib/commons-digester-1.8.jar:/csh/link/hbase/lib/commons-el-1.0.jar:/csh/link/hbase/lib/commons-fileupload-1.2.jar:/csh/link/hbase/lib/commons-httpclient-3.1.jar:/csh/link/hbase/lib/commons-io-2.4.jar:/csh/link/hbase/lib/commons-lang-2.6.jar:/csh/link/hbase/lib/commons-logging-1.2.jar:/csh/link/hbase/lib/commons-math-2.2.jar:/csh/link/hbase/lib/commons-math3-3.1.1.jar:/csh/link/hbase/lib/commons-net-3.1.jar:/csh/link/hbase/lib/disruptor-3.3.0.jar:/csh/link/hbase/lib/esapi-2.1.0.jar:/csh/link/hbase/lib/findbugs-annotations-1.3.9-1.jar:/csh/link/hbase/lib/guava-12.0.1.jar:/csh/link/hbase/lib/guice-3.0.jar:/csh/link/hbase/lib/guice-servlet-3.0.jar:/csh/link/hbase/lib/hadoop-annotations-2.5.1.jar:/csh/link/hbase/lib/hadoop-auth-2.5.1.jar:/csh/link/hbase/lib/hadoop-client-2.5.1.jar:/csh/link/hbase/lib/hadoop-common-2.5.1.jar:/csh/link/hbase/lib/hadoop-hdfs-2.5.1.jar:/csh/link/hbase/lib/hadoop-mapreduce-client-app-2.5.1.jar:/csh/link/hbase/lib/hadoop-mapreduce-client-common-2.5.1.jar:/csh/link/hbase/lib/hadoop-mapreduce-client-core-2.5.1.jar:/csh/link/hbase/lib/hadoop-mapreduce-client-jobclient-2.5.1.jar:/csh/link/hbase/lib/hadoop-mapreduce-client-shuffle-2.5.1.jar:/csh/link/hbase/lib/hadoop-yarn-api-2.5.1.jar:/csh/link/hbase/lib/hadoop-yarn-client-2.5.1.jar:/csh/link/hbase/lib/hadoop-yarn-common-2.5.1.jar:/csh/link/hbase/lib/hadoop-yarn-server-common-2.5.1.jar:/csh/link/hbase/lib/hbase-annotations-1.1.4.jar:/csh/link/hbase/lib/hbase-annotations-1.1.4-tests.jar:/csh/link/hbase/lib/hbase-client-1.1.4.jar:/csh/link/hbase/lib/hbase-common-1.1.4.jar:/csh/link/hbase/lib/hbase-common-1.1.4-tests.jar:/csh/link/hbase/lib/hbase-examples-1.1.4.jar:/csh/link/hbase/lib/hbase-hadoop2-compat-1.1.4.jar:/csh/link/hbase/lib/hbase-hadoop-compat-1.1.4.jar:/csh/link/hbase/lib/hbase-it-1.1.4.jar:/csh/link/hbase/lib/hbase-it-1.1.4-tests.jar:/csh/link/hbase/lib/hbase-prefix-tree-1.1.4.jar:/csh/link/hbase/lib/hbase-procedure-1.1.4.jar:/csh/link/hbase/lib/hbase-protocol-1.1.4.jar:/csh/link/hbase/lib/hbase-resource-bundle-1.1.4.jar:/csh/link/hbase/lib/hbase-rest-1.1.4.jar:/csh/link/hbase/lib/hbase-server-1.1.4.jar:/csh/link/hbase/lib/hbase-server-1.1.4-tests.jar:/csh/link/hbase/lib/hbase-shell-1.1.4.jar:/csh/link/hbase/lib/hbase-thrift-1.1.4.jar:/csh/link/hbase/lib/htrace-core-3.1.0-incubating.jar:/csh/link/hbase/lib/httpclient-4.2.5.jar:/csh/link/hbase/lib/httpcore-4.1.3.jar:/csh/link/hbase/lib/jackson-core-asl-1.9.13.jar:/csh/link/hbase/lib/jackson-jaxrs-1.9.13.jar:/csh/link/hbase/lib/jackson-mapper-asl-1.9.13.jar:/csh/link/hbase/lib/jackson-xc-1.9.13.jar:/csh/link/hbase/lib/jamon-runtime-2.3.1.jar:/csh/link/hbase/lib/jasper-compiler-5.5.23.jar:/csh/link/hbase/lib/jasper-runtime-5.5.23.jar:/csh/link/hbase/lib/javax.inject-1.jar:/csh/link/hbase/lib/java-xmlbuilder-0.4.jar:/csh/link/hbase/lib/jaxb-api-2.2.2.jar:/csh/link/hbase/lib/jaxb-impl-2.2.3-1.jar:/csh/link/hbase/lib/jcodings-1.0.8.jar:/csh/link/hbase/lib/jersey-client-1.9.jar:/csh/link/hbase/lib/jersey-core-1.9.jar:/csh/link/hbase/lib/jersey-guice-1.9.jar:/csh/link/hbase/lib/jersey-json-1.9.jar:/csh/link/hbase/lib/jersey-server-1.9.jar:/csh/link/hbase/lib/jets3t-0.9.0.jar:/csh/link/hbase/lib/jettison-1.3.3.jar:/csh/link/hbase/lib/jetty-6.1.26.jar:/csh/link/hbase/lib/jetty-sslengine-6.1.26.jar:/csh/link/hbase/lib/jetty-util-6.1.26.jar:/csh/link/hbase/lib/joni-2.1.2.jar:/csh/link/hbase/lib/jruby-complete-1.6.8.jar:/csh/link/hbase/lib/jsch-0.1.42.jar:/csh/link/hbase/lib/jsp-2.1-6.1.14.jar:/csh/link/hbase/lib/jsp-api-2.1-6.1.14.jar:/csh/link/hbase/lib/jsr305-1.3.9.jar:/csh/link/hbase/lib/junit-4.12.jar:/csh/link/hbase/lib/leveldbjni-all-1.8.jar:/csh/link/hbase/lib/libthrift-0.9.0.jar:/csh/link/hbase/lib/log4j-1.2.17.jar:/csh/link/hbase/lib/metrics-core-2.2.0.jar:/csh/link/hbase/lib/nekohtml-1.9.12.jar:/csh/link/hbase/lib/netty-3.2.4.Final.jar:/csh/link/hbase/lib/netty-all-4.0.23.Final.jar:/csh/link/hbase/lib/paranamer-2.3.jar:/csh/link/hbase/lib/protobuf-java-2.5.0.jar:/csh/link/hbase/lib/servlet-api-2.5-6.1.14.jar:/csh/link/hbase/lib/servlet-api-2.5.jar:/csh/link/hbase/lib/snappy-java-1.0.4.1.jar:/csh/link/hbase/lib/spymemcached-2.11.6.jar:/csh/link/hbase/lib/xalan-2.7.0.jar:/csh/link/hbase/lib/xml-apis-1.3.03.jar:/csh/link/hbase/lib/xml-apis-ext-1.3.04.jar:/csh/link/hbase/lib/xmlenc-0.52.jar:/csh/link/hbase/lib/xom-1.2.5.jar:/csh/link/hbase/lib/xz-1.0.jar:/csh/link/hbase/lib/zookeeper-3.4.6.jar:/csh/software/hadoop-2.7.2/etc/hadoop:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/activation-1.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/asm-3.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/avro-1.7.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-cli-1.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-codec-1.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-collections-3.2.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-compress-1.4.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-configuration-1.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-digester-1.8.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-httpclient-3.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-io-2.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-lang-2.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/commons-net-3.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/curator-client-2.7.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/curator-framework-2.7.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/gson-2.2.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/guava-11.0.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/hadoop-annotations-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/hadoop-auth-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/hamcrest-core-1.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/httpclient-4.2.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/httpcore-4.2.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jersey-core-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jersey-json-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jersey-server-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jets3t-0.9.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jettison-1.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jetty-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jetty-util-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jsch-0.1.42.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jsp-api-2.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/jsr305-3.0.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/junit-4.11.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/log4j-1.2.17.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/mockito-all-1.8.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/netty-3.6.2.Final.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/paranamer-2.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/servlet-api-2.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/stax-api-1.0-2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/xmlenc-0.52.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/xz-1.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/zookeeper-3.4.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/hadoop-common-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/hadoop-common-2.7.2-tests.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/hadoop-nfs-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/common/jdiff:/csh/software/hadoop-2.7.2/share/hadoop/common/lib:/csh/software/hadoop-2.7.2/share/hadoop/common/sources:/csh/software/hadoop-2.7.2/share/hadoop/common/templates:/csh/software/hadoop-2.7.2/share/hadoop/hdfs:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/asm-3.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-io-2.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/guava-11.0.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/hadoop-hdfs-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/hadoop-hdfs-2.7.2-tests.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/jdiff:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/lib:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/sources:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/templates:/csh/software/hadoop-2.7.2/share/hadoop/hdfs/webapps:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/activation-1.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/asm-3.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-cli-1.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-codec-1.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-io-2.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/guava-11.0.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/guice-3.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/javax.inject-1.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jersey-client-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jersey-core-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jersey-json-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jersey-server-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jettison-1.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jetty-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/log4j-1.2.17.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/servlet-api-2.5.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/xz-1.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-api-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-client-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-common-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-registry-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-common-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/yarn/lib:/csh/software/hadoop-2.7.2/share/hadoop/yarn/sources:/csh/software/hadoop-2.7.2/share/hadoop/yarn/test:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/asm-3.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/guice-3.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/javax.inject-1.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib/xz-1.0.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/lib-examples:/csh/software/hadoop-2.7.2/share/hadoop/mapreduce/sources:/csh/link/hadoop/contrib/capacity-scheduler/*.jar:/csh/link/hbase/conf:/csh/link/hive/lib/*&apos; -Djava.library.path=:/csh/software/hadoop-2.7.2/lib/native:/csh/software/hadoop-2.7.2/lib/native org.apache.flume.node.Application --conf-file conf/single-node.conf --name a1</span><br><span class="line">2016-04-19 21:44:11,126 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start(PollingPropertiesFileConfigurationProvider.java:61)] Configuration provider starting</span><br><span class="line">2016-04-19 21:44:11,165 (conf-file-poller-0) [INFO - org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:133)] Reloading configuration file:conf/single-node.conf</span><br><span class="line">2016-04-19 21:44:11,200 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:931)] Added sinks: k1 Agent: a1</span><br><span class="line">2016-04-19 21:44:11,200 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)] Processing:k1</span><br><span class="line">2016-04-19 21:44:11,201 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)] Processing:k1</span><br><span class="line">2016-04-19 21:44:11,285 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration.validateConfiguration(FlumeConfiguration.java:141)] Post-validation flume configuration contains configuration for agents: [a1]</span><br><span class="line">2016-04-19 21:44:11,285 (conf-file-poller-0) [INFO - org.apache.flume.node.AbstractConfigurationProvider.loadChannels(AbstractConfigurationProvider.java:145)] Creating channels</span><br><span class="line">2016-04-19 21:44:11,341 (conf-file-poller-0) [INFO - org.apache.flume.channel.DefaultChannelFactory.create(DefaultChannelFactory.java:42)] Creating instance of channel c1 type memory</span><br><span class="line">2016-04-19 21:44:11,506 (conf-file-poller-0) [INFO - org.apache.flume.node.AbstractConfigurationProvider.loadChannels(AbstractConfigurationProvider.java:200)] Created channel c1</span><br><span class="line">2016-04-19 21:44:11,507 (conf-file-poller-0) [INFO - org.apache.flume.source.DefaultSourceFactory.create(DefaultSourceFactory.java:41)] Creating instance of source r1, type netcat</span><br><span class="line">2016-04-19 21:44:11,590 (conf-file-poller-0) [INFO - org.apache.flume.sink.DefaultSinkFactory.create(DefaultSinkFactory.java:42)] Creating instance of sink: k1, type: logger</span><br><span class="line">2016-04-19 21:44:11,610 (conf-file-poller-0) [INFO - org.apache.flume.node.AbstractConfigurationProvider.getConfiguration(AbstractConfigurationProvider.java:114)] Channel c1 connected to [r1, k1]</span><br><span class="line">2016-04-19 21:44:11,684 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:138)] Starting new configuration:&#123; sourceRunners:&#123;r1=EventDrivenSourceRunner: &#123; source:org.apache.flume.source.NetcatSource&#123;name:r1,state:IDLE&#125; &#125;&#125; sinkRunners:&#123;k1=SinkRunner: &#123; policy:org.apache.flume.sink.DefaultSinkProcessor@7ceacc49 counterGroup:&#123; name:null counters:&#123;&#125; &#125; &#125;&#125; channels:&#123;c1=org.apache.flume.channel.MemoryChannel&#123;name: c1&#125;&#125; &#125;</span><br><span class="line">2016-04-19 21:44:11,713 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:145)] Starting Channel c1</span><br><span class="line">2016-04-19 21:44:17,439 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:120)] Monitored counter group for type: CHANNEL, name: c1: Successfully registered new MBean.</span><br><span class="line">2016-04-19 21:44:17,440 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:96)] Component type: CHANNEL, name: c1 started</span><br><span class="line">2016-04-19 21:44:17,452 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:173)] Starting Sink k1</span><br><span class="line">2016-04-19 21:44:17,462 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:184)] Starting Source r1</span><br><span class="line">2016-04-19 21:44:17,474 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.source.NetcatSource.start(NetcatSource.java:150)] Source starting</span><br><span class="line">2016-04-19 21:44:18,012 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.source.NetcatSource.start(NetcatSource.java:164)] Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444]</span><br></pre></td></tr></table></figure></p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>在XShell中打开另一个窗口输入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# telnet 127.0.0.1 44444</span><br><span class="line">Trying 127.0.0.1...</span><br><span class="line">Connected to 127.0.0.1.</span><br><span class="line">Escape character is &apos;^]&apos;.</span><br><span class="line">hello flume!</span><br><span class="line">OK</span><br><span class="line">hello big data</span><br><span class="line">OK</span><br></pre></td></tr></table></figure>
<p>在原来启动flume的命令行中可以看到：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2016-04-19 21:47:35,653 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:94)] Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F 20 66 6C 75 6D 65 21 0D          hello flume!. &#125;</span><br><span class="line">2016-04-19 21:49:10,610 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:94)] Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F 20 62 69 67 20 64 61 74 61 0D    hello big data. &#125;</span><br></pre></td></tr></table></figure>
          
        
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/04/21/Sqoop HDFS导入到MySQL/" itemprop="url">
                  Sqoop HDFS导入到MySQL
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-04-21T16:28:01+08:00" content="2016-04-21">
              2016-04-21
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Sqoop/" itemprop="url" rel="index">
                    <span itemprop="name">Sqoop</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/04/21/Sqoop HDFS导入到MySQL/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/04/21/Sqoop HDFS导入到MySQL/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/04/21/Sqoop HDFS导入到MySQL/" class="leancloud_visitors" data-flag-title="Sqoop HDFS导入到MySQL">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="在MySQL中创建表"><a href="#在MySQL中创建表" class="headerlink" title="在MySQL中创建表"></a>在MySQL中创建表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE `tree1` (</span><br><span class="line">  `id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `treeNumber` varchar(100) NOT NULL,</span><br><span class="line">  `productinformationId` int(11) NOT NULL,</span><br><span class="line">  PRIMARY KEY (`id`)</span><br><span class="line">) DEFAULT CHARSET=utf8;</span><br></pre></td></tr></table></figure>
<h3 id="执行命令"><a href="#执行命令" class="headerlink" title="执行命令"></a>执行命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop export --connect jdbc:mysql://node1:3306/phx \--username root \--table tree1 -m 1 \--export-dir /sqoop/tree2</span><br></pre></td></tr></table></figure>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 conf]# sqoop export --connect jdbc:mysql://node1:3306/phx \--username root \--table tree1 -m 1 \--export-dir /sqoop/tree2</span><br><span class="line">Warning: /csh/link/sqoop/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /csh/link/sqoop/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">16/04/19 01:10:23 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6</span><br><span class="line">16/04/19 01:10:23 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">16/04/19 01:10:23 INFO tool.CodeGenTool: Beginning code generation</span><br><span class="line">16/04/19 01:10:24 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `tree1` AS t LIMIT 1</span><br><span class="line">16/04/19 01:10:24 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `tree1` AS t LIMIT 1</span><br><span class="line">16/04/19 01:10:24 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /csh/link/hadoop</span><br><span class="line">Note: /tmp/sqoop-root/compile/8efdea8893437a1359c77de6d1bd2395/tree1.java uses or overrides a deprecated API.</span><br><span class="line">Note: Recompile with -Xlint:deprecation for details.</span><br><span class="line">16/04/19 01:10:28 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/8efdea8893437a1359c77de6d1bd2395/tree1.jar</span><br><span class="line">16/04/19 01:10:29 INFO mapreduce.ExportJobBase: Beginning export of tree1</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/csh/software/hbase-1.1.4/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]</span><br><span class="line">16/04/19 01:10:30 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar</span><br><span class="line">16/04/19 01:10:38 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative</span><br><span class="line">16/04/19 01:10:38 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative</span><br><span class="line">16/04/19 01:10:38 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps</span><br><span class="line">16/04/19 01:10:38 INFO client.RMProxy: Connecting to ResourceManager at node1/192.168.161.11:8032</span><br><span class="line">16/04/19 01:10:48 INFO input.FileInputFormat: Total input paths to process : 1</span><br><span class="line">16/04/19 01:10:48 INFO input.FileInputFormat: Total input paths to process : 1</span><br><span class="line">16/04/19 01:10:49 INFO mapreduce.JobSubmitter: number of splits:1</span><br><span class="line">16/04/19 01:10:49 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative</span><br><span class="line">16/04/19 01:10:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1461026006811_0003</span><br><span class="line">16/04/19 01:10:51 INFO impl.YarnClientImpl: Submitted application application_1461026006811_0003</span><br><span class="line">16/04/19 01:10:51 INFO mapreduce.Job: The url to track the job: http://node1:8088/proxy/application_1461026006811_0003/</span><br><span class="line">16/04/19 01:10:51 INFO mapreduce.Job: Running job: job_1461026006811_0003</span><br><span class="line">16/04/19 01:11:33 INFO mapreduce.Job: Job job_1461026006811_0003 running in uber mode : false</span><br><span class="line">16/04/19 01:11:33 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">16/04/19 01:12:24 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">16/04/19 01:12:27 INFO mapreduce.Job: Job job_1461026006811_0003 completed successfully</span><br><span class="line">16/04/19 01:12:27 INFO mapreduce.Job: Counters: 30</span><br><span class="line">	File System Counters</span><br><span class="line">		FILE: Number of bytes read=0</span><br><span class="line">		FILE: Number of bytes written=137080</span><br><span class="line">		FILE: Number of read operations=0</span><br><span class="line">		FILE: Number of large read operations=0</span><br><span class="line">		FILE: Number of write operations=0</span><br><span class="line">		HDFS: Number of bytes read=251</span><br><span class="line">		HDFS: Number of bytes written=0</span><br><span class="line">		HDFS: Number of read operations=4</span><br><span class="line">		HDFS: Number of large read operations=0</span><br><span class="line">		HDFS: Number of write operations=0</span><br><span class="line">	Job Counters </span><br><span class="line">		Launched map tasks=1</span><br><span class="line">		Rack-local map tasks=1</span><br><span class="line">		Total time spent by all maps in occupied slots (ms)=45106</span><br><span class="line">		Total time spent by all reduces in occupied slots (ms)=0</span><br><span class="line">		Total time spent by all map tasks (ms)=45106</span><br><span class="line">		Total vcore-milliseconds taken by all map tasks=45106</span><br><span class="line">		Total megabyte-milliseconds taken by all map tasks=46188544</span><br><span class="line">	Map-Reduce Framework</span><br><span class="line">		Map input records=7</span><br><span class="line">		Map output records=7</span><br><span class="line">		Input split bytes=122</span><br><span class="line">		Spilled Records=0</span><br><span class="line">		Failed Shuffles=0</span><br><span class="line">		Merged Map outputs=0</span><br><span class="line">		GC time elapsed (ms)=258</span><br><span class="line">		CPU time spent (ms)=1680</span><br><span class="line">		Physical memory (bytes) snapshot=86806528</span><br><span class="line">		Virtual memory (bytes) snapshot=2086785024</span><br><span class="line">		Total committed heap usage (bytes)=17235968</span><br><span class="line">	File Input Format Counters </span><br><span class="line">		Bytes Read=0</span><br><span class="line">	File Output Format Counters </span><br><span class="line">		Bytes Written=0</span><br><span class="line">16/04/19 01:12:27 INFO mapreduce.ExportJobBase: Transferred 251 bytes in 109.7035 seconds (2.288 bytes/sec)</span><br><span class="line">16/04/19 01:12:27 INFO mapreduce.ExportJobBase: Exported 7 records.</span><br></pre></td></tr></table></figure>
<h3 id="在MySQL中查看tree1表中数据"><a href="#在MySQL中查看tree1表中数据" class="headerlink" title="在MySQL中查看tree1表中数据"></a>在MySQL中查看tree1表中数据</h3><p><img src="https://1csh1.github.io/img/Sqoop%20HDFS导入到MySQL/数据库表.jpg" alt="数据库表"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/04/21/Sqoop MySQL 导入到Hive/" itemprop="url">
                  Sqoop MySQL 导入到Hive
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-04-21T15:52:20+08:00" content="2016-04-21">
              2016-04-21
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Sqoop/" itemprop="url" rel="index">
                    <span itemprop="name">Sqoop</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/04/21/Sqoop MySQL 导入到Hive/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/04/21/Sqoop MySQL 导入到Hive/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/04/21/Sqoop MySQL 导入到Hive/" class="leancloud_visitors" data-flag-title="Sqoop MySQL 导入到Hive">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="将数据库phx中的tree表的数据导入到Hive中"><a href="#将数据库phx中的tree表的数据导入到Hive中" class="headerlink" title="将数据库phx中的tree表的数据导入到Hive中"></a>将数据库phx中的tree表的数据导入到Hive中</h3><p>命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop import --connect jdbc:mysql://node1:3306/phx \--username root \--table tree \--hive-import \--hive-overwrite \--create-hive-table \--hive-table tree1 \--target-dir /sqoop/tree3</span><br></pre></td></tr></table></figure>
<p>结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 conf]# sqoop import --connect jdbc:mysql://node1:3306/phx \--username root \--table tree \--hive-import \--hive-overwrite \--create-hive-table \--hive-table tree1 \--target-dir /sqoop/tree3</span><br><span class="line">Warning: /csh/link/sqoop/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /csh/link/sqoop/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">16/04/18 21:20:00 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6</span><br><span class="line">16/04/18 21:20:00 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override</span><br><span class="line">16/04/18 21:20:00 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.</span><br><span class="line">16/04/18 21:20:00 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">16/04/18 21:20:00 INFO tool.CodeGenTool: Beginning code generation</span><br><span class="line">16/04/18 21:20:00 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `tree` AS t LIMIT 1</span><br><span class="line">16/04/18 21:20:00 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `tree` AS t LIMIT 1</span><br><span class="line">16/04/18 21:20:00 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /csh/link/hadoop</span><br><span class="line">Note: /tmp/sqoop-root/compile/9cff004a8ff405b712c978864c2775df/tree.java uses or overrides a deprecated API.</span><br><span class="line">Note: Recompile with -Xlint:deprecation for details.</span><br><span class="line">16/04/18 21:20:03 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/9cff004a8ff405b712c978864c2775df/tree.jar</span><br><span class="line">16/04/18 21:20:03 WARN manager.MySQLManager: It looks like you are importing from mysql.</span><br><span class="line">16/04/18 21:20:03 WARN manager.MySQLManager: This transfer can be faster! Use the --direct</span><br><span class="line">16/04/18 21:20:03 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.</span><br><span class="line">16/04/18 21:20:03 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)</span><br><span class="line">16/04/18 21:20:03 INFO mapreduce.ImportJobBase: Beginning import of tree</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/csh/software/hbase-1.1.4/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]</span><br><span class="line">16/04/18 21:20:04 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar</span><br><span class="line">16/04/18 21:20:09 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps</span><br><span class="line">16/04/18 21:20:10 INFO client.RMProxy: Connecting to ResourceManager at node1/192.168.161.11:8032</span><br><span class="line">16/04/18 21:20:23 INFO db.DBInputFormat: Using read commited transaction isolation</span><br><span class="line">16/04/18 21:20:23 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `tree`</span><br><span class="line">16/04/18 21:20:24 INFO mapreduce.JobSubmitter: number of splits:4</span><br><span class="line">16/04/18 21:20:25 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1461026006811_0001</span><br><span class="line">16/04/18 21:20:27 INFO impl.YarnClientImpl: Submitted application application_1461026006811_0001</span><br><span class="line">16/04/18 21:20:28 INFO mapreduce.Job: The url to track the job: http://node1:8088/proxy/application_1461026006811_0001/</span><br><span class="line">16/04/18 21:20:28 INFO mapreduce.Job: Running job: job_1461026006811_0001</span><br><span class="line">16/04/18 21:21:01 INFO mapreduce.Job: Job job_1461026006811_0001 running in uber mode : false</span><br><span class="line">16/04/18 21:21:01 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">16/04/18 21:22:00 INFO mapreduce.Job:  map 25% reduce 0%</span><br><span class="line">16/04/18 21:23:13 INFO mapreduce.Job:  map 50% reduce 0%</span><br><span class="line">16/04/18 21:23:14 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">16/04/18 21:23:21 INFO mapreduce.Job: Job job_1461026006811_0001 completed successfully</span><br><span class="line">16/04/18 21:23:22 INFO mapreduce.Job: Counters: 31</span><br><span class="line">	File System Counters</span><br><span class="line">		FILE: Number of bytes read=0</span><br><span class="line">		FILE: Number of bytes written=549568</span><br><span class="line">		FILE: Number of read operations=0</span><br><span class="line">		FILE: Number of large read operations=0</span><br><span class="line">		FILE: Number of write operations=0</span><br><span class="line">		HDFS: Number of bytes read=398</span><br><span class="line">		HDFS: Number of bytes written=373</span><br><span class="line">		HDFS: Number of read operations=16</span><br><span class="line">		HDFS: Number of large read operations=0</span><br><span class="line">		HDFS: Number of write operations=8</span><br><span class="line">	Job Counters </span><br><span class="line">		Killed map tasks=4</span><br><span class="line">		Launched map tasks=7</span><br><span class="line">		Other local map tasks=7</span><br><span class="line">		Total time spent by all maps in occupied slots (ms)=585524</span><br><span class="line">		Total time spent by all reduces in occupied slots (ms)=0</span><br><span class="line">		Total time spent by all map tasks (ms)=585524</span><br><span class="line">		Total vcore-milliseconds taken by all map tasks=585524</span><br><span class="line">		Total megabyte-milliseconds taken by all map tasks=599576576</span><br><span class="line">	Map-Reduce Framework</span><br><span class="line">		Map input records=21</span><br><span class="line">		Map output records=21</span><br><span class="line">		Input split bytes=398</span><br><span class="line">		Spilled Records=0</span><br><span class="line">		Failed Shuffles=0</span><br><span class="line">		Merged Map outputs=0</span><br><span class="line">		GC time elapsed (ms)=722</span><br><span class="line">		CPU time spent (ms)=15670</span><br><span class="line">		Physical memory (bytes) snapshot=394702848</span><br><span class="line">		Virtual memory (bytes) snapshot=8350515200</span><br><span class="line">		Total committed heap usage (bytes)=68792320</span><br><span class="line">	File Input Format Counters </span><br><span class="line">		Bytes Read=0</span><br><span class="line">	File Output Format Counters </span><br><span class="line">		Bytes Written=373</span><br><span class="line">16/04/18 21:23:22 INFO mapreduce.ImportJobBase: Transferred 373 bytes in 193.0206 seconds (1.9324 bytes/sec)</span><br><span class="line">16/04/18 21:23:22 INFO mapreduce.ImportJobBase: Retrieved 21 records.</span><br><span class="line">16/04/18 21:23:23 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `tree` AS t LIMIT 1</span><br><span class="line">16/04/18 21:23:23 INFO hive.HiveImport: Loading uploaded data into Hive</span><br><span class="line">16/04/18 21:25:54 INFO hive.HiveImport: </span><br><span class="line">16/04/18 21:25:54 INFO hive.HiveImport: Logging initialized using configuration in file:/csh/software/apache-hive-1.2.1-bin/conf/hive-log4j.properties</span><br><span class="line">16/04/18 21:25:57 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">16/04/18 21:25:57 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">16/04/18 21:25:57 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/csh/software/hbase-1.1.4/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">16/04/18 21:25:57 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">16/04/18 21:25:57 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]</span><br><span class="line">16/04/18 21:27:27 INFO hive.HiveImport: OK</span><br><span class="line">16/04/18 21:27:28 INFO hive.HiveImport: Time taken: 23.5 seconds</span><br><span class="line">16/04/18 21:27:30 INFO hive.HiveImport: Loading data to table default.tree1</span><br><span class="line">16/04/18 21:27:35 INFO hive.HiveImport: Table default.tree1 stats: [numFiles=4, numRows=0, totalSize=373, rawDataSize=0]</span><br><span class="line">16/04/18 21:27:35 INFO hive.HiveImport: OK</span><br><span class="line">16/04/18 21:27:35 INFO hive.HiveImport: Time taken: 8.27 seconds</span><br><span class="line">16/04/18 21:27:36 INFO hive.HiveImport: Hive import complete.</span><br><span class="line">16/04/18 21:27:40 INFO hive.HiveImport: Export directory is contains the _SUCCESS file only, removing the directory.</span><br></pre></td></tr></table></figure>
<p>浏览HDFS如下图：<br><img src="https://1csh1.github.io/img/Sqoop%20MySQL%20导入到Hive/HDFS.jpg" alt="HDFS"></p>
<p>在Hive中检查是否有数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from tree1;</span><br><span class="line">OK</span><br><span class="line">1	466464684640	1</span><br><span class="line">2	466464684641	2</span><br><span class="line">3	466464684642	3</span><br><span class="line">4	466464684643	1</span><br><span class="line">5	466464684644	2</span><br><span class="line">6	466464684645	3</span><br><span class="line">7	466464684646	1</span><br><span class="line">8	466464684647	2</span><br><span class="line">9	466464684648	3</span><br><span class="line">10	466464684649	1</span><br><span class="line">11	4664646846410	2</span><br><span class="line">12	4664646846411	3</span><br><span class="line">13	4664646846412	1</span><br><span class="line">14	4664646846413	2</span><br><span class="line">15	4664646846414	3</span><br><span class="line">16	4664646846415	1</span><br><span class="line">17	4664646846416	2</span><br><span class="line">18	4664646846417	3</span><br><span class="line">19	4664646846418	1</span><br><span class="line">20	4664646846419	2</span><br><span class="line">21	111111	1</span><br><span class="line">Time taken: 2.622 seconds, Fetched: 21 row(s)</span><br></pre></td></tr></table></figure>
<h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><table>
<thead>
<tr>
<th style="text-align:left">参数</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">–hive-home <dir></dir></td>
<td style="text-align:left">Hive的安装目录，可以通过该参数覆盖掉默认的hive目录</td>
</tr>
<tr>
<td style="text-align:left">–hive-overwrite</td>
<td style="text-align:left">覆盖掉在hive表中已经存在的数据</td>
</tr>
<tr>
<td style="text-align:left">–create-hive-table</td>
<td style="text-align:left">默认是false，如果目标表已经存在了，那么创建任务会失败</td>
</tr>
<tr>
<td style="text-align:left">–hive-table</td>
<td style="text-align:left">后面接要创建的hive表</td>
</tr>
<tr>
<td style="text-align:left">–table</td>
<td style="text-align:left">指定关系数据库表名</td>
</tr>
</tbody>
</table>

          
        
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/04/21/Sqoop MySQL 导入到HDFS/" itemprop="url">
                  Sqoop MySQL 导入到HDFS
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-04-21T15:30:17+08:00" content="2016-04-21">
              2016-04-21
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Sqoop/" itemprop="url" rel="index">
                    <span itemprop="name">Sqoop</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/04/21/Sqoop MySQL 导入到HDFS/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/04/21/Sqoop MySQL 导入到HDFS/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/04/21/Sqoop MySQL 导入到HDFS/" class="leancloud_visitors" data-flag-title="Sqoop MySQL 导入到HDFS">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="从MySQL数据库phx中读取tree表到HDFS"><a href="#从MySQL数据库phx中读取tree表到HDFS" class="headerlink" title="从MySQL数据库phx中读取tree表到HDFS"></a>从MySQL数据库phx中读取tree表到HDFS</h3><p>执行命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop import --connect jdbc:mysql://node1:3306/phx \--username root --table tree --m 1</span><br></pre></td></tr></table></figure></p>
<p>命令行输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">Warning: /csh/link/sqoop/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /csh/link/sqoop/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">16/04/10 07:14:24 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6</span><br><span class="line">16/04/10 07:14:25 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">16/04/10 07:14:25 INFO tool.CodeGenTool: Beginning code generation</span><br><span class="line">16/04/10 07:14:25 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `tree` AS t LIMIT 1</span><br><span class="line">16/04/10 07:14:26 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `tree` AS t LIMIT 1</span><br><span class="line">16/04/10 07:14:26 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /csh/link/hadoop</span><br><span class="line">Note: /tmp/sqoop-root/compile/7c9c9a4ffecefa757a4f2073109cc801/tree.java uses or overrides a deprecated API.</span><br><span class="line">Note: Recompile with -Xlint:deprecation for details.</span><br><span class="line">16/04/10 07:14:29 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/7c9c9a4ffecefa757a4f2073109cc801/tree.jar</span><br><span class="line">16/04/10 07:14:30 WARN manager.MySQLManager: It looks like you are importing from mysql.</span><br><span class="line">16/04/10 07:14:30 WARN manager.MySQLManager: This transfer can be faster! Use the --direct</span><br><span class="line">16/04/10 07:14:30 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.</span><br><span class="line">16/04/10 07:14:30 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)</span><br><span class="line">16/04/10 07:14:30 INFO mapreduce.ImportJobBase: Beginning import of tree</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/csh/software/hadoop-2.7.2/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/csh/software/hbase-1.1.4/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]</span><br><span class="line">16/04/10 07:14:30 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar</span><br><span class="line">16/04/10 07:14:32 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps</span><br><span class="line">16/04/10 07:14:32 INFO client.RMProxy: Connecting to ResourceManager at node1/192.168.161.11:8032</span><br><span class="line">16/04/10 07:14:43 INFO db.DBInputFormat: Using read commited transaction isolation</span><br><span class="line">16/04/10 07:14:43 INFO mapreduce.JobSubmitter: number of splits:1</span><br><span class="line">16/04/10 07:14:44 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1460265828636_0005</span><br><span class="line">16/04/10 07:14:50 INFO impl.YarnClientImpl: Submitted application application_1460265828636_0005</span><br><span class="line">16/04/10 07:14:50 INFO mapreduce.Job: The url to track the job: http://node1:8088/proxy/application_1460265828636_0005/</span><br><span class="line">16/04/10 07:14:50 INFO mapreduce.Job: Running job: job_1460265828636_0005</span><br><span class="line">16/04/10 07:15:29 INFO mapreduce.Job: Job job_1460265828636_0005 running in uber mode : false</span><br><span class="line">16/04/10 07:15:29 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">16/04/10 07:16:15 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">16/04/10 07:16:17 INFO mapreduce.Job: Job job_1460265828636_0005 completed successfully</span><br><span class="line">16/04/10 07:16:17 INFO mapreduce.Job: Counters: 30</span><br><span class="line">	File System Counters</span><br><span class="line">		FILE: Number of bytes read=0</span><br><span class="line">		FILE: Number of bytes written=137395</span><br><span class="line">		FILE: Number of read operations=0</span><br><span class="line">		FILE: Number of large read operations=0</span><br><span class="line">		FILE: Number of write operations=0</span><br><span class="line">		HDFS: Number of bytes read=87</span><br><span class="line">		HDFS: Number of bytes written=361</span><br><span class="line">		HDFS: Number of read operations=4</span><br><span class="line">		HDFS: Number of large read operations=0</span><br><span class="line">		HDFS: Number of write operations=2</span><br><span class="line">	Job Counters </span><br><span class="line">		Launched map tasks=1</span><br><span class="line">		Other local map tasks=1</span><br><span class="line">		Total time spent by all maps in occupied slots (ms)=42224</span><br><span class="line">		Total time spent by all reduces in occupied slots (ms)=0</span><br><span class="line">		Total time spent by all map tasks (ms)=42224</span><br><span class="line">		Total vcore-milliseconds taken by all map tasks=42224</span><br><span class="line">		Total megabyte-milliseconds taken by all map tasks=43237376</span><br><span class="line">	Map-Reduce Framework</span><br><span class="line">		Map input records=20</span><br><span class="line">		Map output records=20</span><br><span class="line">		Input split bytes=87</span><br><span class="line">		Spilled Records=0</span><br><span class="line">		Failed Shuffles=0</span><br><span class="line">		Merged Map outputs=0</span><br><span class="line">		GC time elapsed (ms)=151</span><br><span class="line">		CPU time spent (ms)=3460</span><br><span class="line">		Physical memory (bytes) snapshot=86683648</span><br><span class="line">		Virtual memory (bytes) snapshot=2083274752</span><br><span class="line">		Total committed heap usage (bytes)=16318464</span><br><span class="line">	File Input Format Counters </span><br><span class="line">		Bytes Read=0</span><br><span class="line">	File Output Format Counters </span><br><span class="line">		Bytes Written=361</span><br><span class="line">16/04/10 07:16:17 INFO mapreduce.ImportJobBase: Transferred 361 bytes in 105.2373 seconds (3.4303 bytes/sec)</span><br><span class="line">16/04/10 07:16:17 INFO mapreduce.ImportJobBase: Retrieved 20 records.</span><br></pre></td></tr></table></figure>
<p>访问 <a href="http://node1:8088" target="_blank" rel="noopener">http://node1:8088</a> 可以看到生成一个MapReduce任务<br><img src="https://1csh1.github.io/img/Sqoop%20MySQL%20导入到HDFS/MapReduce任务1.jpg" alt=""><br>结果：<br><img src="https://1csh1.github.io/img/Sqoop%20MySQL%20导入到HDFS/结果1.jpg" alt="这里写图片描述"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 sbin]# hadoop fs -cat /user/root/tree/p*</span><br><span class="line">1,466464684640,1</span><br><span class="line">2,466464684641,2</span><br><span class="line">3,466464684642,3</span><br><span class="line">4,466464684643,1</span><br><span class="line">5,466464684644,2</span><br><span class="line">6,466464684645,3</span><br><span class="line">7,466464684646,1</span><br><span class="line">8,466464684647,2</span><br><span class="line">9,466464684648,3</span><br><span class="line">10,466464684649,1</span><br><span class="line">11,4664646846410,2</span><br><span class="line">12,4664646846411,3</span><br><span class="line">13,4664646846412,1</span><br><span class="line">14,4664646846413,2</span><br><span class="line">15,4664646846414,3</span><br><span class="line">16,4664646846415,1</span><br><span class="line">17,4664646846416,2</span><br><span class="line">18,4664646846417,3</span><br><span class="line">19,4664646846418,1</span><br><span class="line">20,4664646846419,2</span><br></pre></td></tr></table></figure>
<h3 id="导入到指定目录"><a href="#导入到指定目录" class="headerlink" title="导入到指定目录"></a>导入到指定目录</h3><p>参数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--target-dir /directory</span><br></pre></td></tr></table></figure></p>
<p>例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop import --connect jdbc:mysql://node1:3306/phx \--username root --table tree --m 1 \--target-dir /sqoop/</span><br></pre></td></tr></table></figure></p>
<p>结果：<br><img src="https://1csh1.github.io/img/Sqoop%20MySQL%20导入到HDFS/结果2.jpg" alt="这里写图片描述"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 sbin]# hadoop fs -cat /sqoop/p*</span><br><span class="line">1,466464684640,1</span><br><span class="line">2,466464684641,2</span><br><span class="line">3,466464684642,3</span><br><span class="line">4,466464684643,1</span><br><span class="line">5,466464684644,2</span><br><span class="line">6,466464684645,3</span><br><span class="line">7,466464684646,1</span><br><span class="line">8,466464684647,2</span><br><span class="line">9,466464684648,3</span><br><span class="line">10,466464684649,1</span><br><span class="line">11,4664646846410,2</span><br><span class="line">12,4664646846411,3</span><br><span class="line">13,4664646846412,1</span><br><span class="line">14,4664646846413,2</span><br><span class="line">15,4664646846414,3</span><br><span class="line">16,4664646846415,1</span><br><span class="line">17,4664646846416,2</span><br><span class="line">18,4664646846417,3</span><br><span class="line">19,4664646846418,1</span><br><span class="line">20,4664646846419,2</span><br></pre></td></tr></table></figure></p>
<h3 id="导入表子集"><a href="#导入表子集" class="headerlink" title="导入表子集"></a>导入表子集</h3><p>参数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--where &lt;condition&gt;</span><br></pre></td></tr></table></figure></p>
<p>例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop import --connect jdbc:mysql://node1:3306/phx \--username root --table tree --m 1 \--where &quot;productinformationId&quot;=&apos;1&apos; \--target-dir /sqoop/tree2/</span><br></pre></td></tr></table></figure></p>
<p>结果：<br><img src="https://1csh1.github.io/img/Sqoop%20MySQL%20导入到HDFS/结果3.jpg" alt="这里写图片描述"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 sbin]# hadoop fs -cat /sqoop/tree2/p*</span><br><span class="line">1,466464684640,1</span><br><span class="line">4,466464684643,1</span><br><span class="line">7,466464684646,1</span><br><span class="line">10,466464684649,1</span><br><span class="line">13,4664646846412,1</span><br><span class="line">16,4664646846415,1</span><br><span class="line">19,4664646846418,1</span><br></pre></td></tr></table></figure>
<p>增量导入（即只导入新添加的那部分数据）<br>参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--incremental &lt;mode&gt; 选择模式，有：append 和 lastmodified</span><br><span class="line">--check-column &lt;column name&gt; 检查那一列的元素，比如 id</span><br><span class="line">--last-value &lt;last check column value&gt; 原来数据最后一条的参数，比如 最后一条id为20，则填20</span><br></pre></td></tr></table></figure>
<p>例如：<br>在数据库表tree中添加一条记录，如下图<br><img src="https://1csh1.github.io/img/Sqoop%20MySQL%20导入到HDFS/数据库表.jpg" alt="这里写图片描述"><br>运行命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop import --connect jdbc:mysql://node1:3306/phx \--username root --table tree --m 1 \--target-dir /sqoop/ \--incremental append \--check-column id \--last-value 20</span><br></pre></td></tr></table></figure>
<p>结果：<br><img src="https://1csh1.github.io/img/Sqoop%20MySQL%20导入到HDFS/结果4.jpg" alt="这里写图片描述"></p>
<h3 id="参数列表"><a href="#参数列表" class="headerlink" title="参数列表"></a>参数列表</h3><table>
<thead>
<tr>
<th style="text-align:left">参数</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">–append</td>
<td style="text-align:left">将数据追加到hdfs中已经存在的dataset中。使用该参数，sqoop将把数据先导入到一个临时目录中，然后重新给文件命名到一个正式的目录中，以避免和该目录中已存在的文件重名。</td>
</tr>
<tr>
<td style="text-align:left">–as-avrodatafile</td>
<td style="text-align:left">将数据导入到一个Avro数据文件中</td>
</tr>
<tr>
<td style="text-align:left">–as-sequencefile</td>
<td style="text-align:left">将数据导入到一个sequence文件中</td>
</tr>
<tr>
<td style="text-align:left">–as-textfile</td>
<td style="text-align:left">将数据导入到一个普通文本文件中，生成该文本文件后，可以在hive中通过sql语句查询出结果。</td>
</tr>
<tr>
<td style="text-align:left">–boundary-query <statement></statement></td>
<td style="text-align:left">边界查询，也就是在导入前先通过SQL查询得到一个结果集，然后导入的数据就是该结果集内的数据，格式如：–boundary-query ‘select id,no from t where id = 3’，表示导入的数据为id=3的记录，或者 select min(<split-by>), max(<split-by>) from <table name="">，注意查询的字段中不能有数据类型为字符串的字段，否则会报错</table></split-by></split-by></td>
</tr>
<tr>
<td style="text-align:left">–columns<col,col></col,col></td>
<td style="text-align:left">指定要导入的字段值，格式如：–columns id,username</td>
</tr>
<tr>
<td style="text-align:left">–direct</td>
<td style="text-align:left">直接导入模式，使用的是关系数据库自带的导入导出工具。官网上是说这样导入会更快</td>
</tr>
<tr>
<td style="text-align:left">–direct-split-size</td>
<td style="text-align:left">在使用上面direct直接导入的基础上，对导入的流按字节数分块，特别是使用直连模式从PostgreSQL导入数据的时候，可以将一个到达设定大小的文件分为几个独立的文件。</td>
</tr>
<tr>
<td style="text-align:left">–inline-lob-limit</td>
<td style="text-align:left">设定大对象数据类型的最大值</td>
</tr>
<tr>
<td style="text-align:left">-m,–num-mappers</td>
<td style="text-align:left">启动N个map来并行导入数据，默认是4个，最好不要将数字设置为高于集群的节点数</td>
</tr>
<tr>
<td style="text-align:left">–query，-e <sql></sql></td>
<td style="text-align:left">从查询结果中导入数据，该参数使用时必须指定–target-dir、–hive-table，在查询语句中一定要有where条件且在where条件中需要包含 \$CONDITIONS，示例：–query ‘select * from t where \$CONDITIONS ‘ –target-dir /tmp/t –hive-table t</td>
</tr>
<tr>
<td style="text-align:left">–split-by <column></column></td>
<td style="text-align:left">表的列名，用来切分工作单元，一般后面跟主键ID</td>
</tr>
<tr>
<td style="text-align:left">–table <table-name></table-name></td>
<td style="text-align:left">关系数据库表名，数据从该表中获取</td>
</tr>
<tr>
<td style="text-align:left">–delete-target-dir</td>
<td style="text-align:left">删除目标目录</td>
</tr>
<tr>
<td style="text-align:left">–target-dir <dir></dir></td>
<td style="text-align:left">指定hdfs路径</td>
</tr>
<tr>
<td style="text-align:left">–warehouse-dir <dir></dir></td>
<td style="text-align:left">与 –target-dir 不能同时使用，指定数据导入的存放目录，适用于hdfs导入，不适合导入hive目录</td>
</tr>
<tr>
<td style="text-align:left">–where</td>
<td style="text-align:left">从关系数据库导入数据时的查询条件，示例：–where “id = 2”</td>
</tr>
<tr>
<td style="text-align:left">-z,–compress</td>
<td style="text-align:left">压缩参数，默认情况下数据是没被压缩的，通过该参数可以使用gzip压缩算法对数据进行压缩，适用于SequenceFile, text文本文件, 和Avro文件</td>
</tr>
<tr>
<td style="text-align:left">–compression-codec</td>
<td style="text-align:left">Hadoop压缩编码，默认是gzip</td>
</tr>
<tr>
<td style="text-align:left">–null-string <null-string></null-string></td>
<td style="text-align:left">可选参数，如果没有指定，则字符串null将被使用</td>
</tr>
<tr>
<td style="text-align:left">–null-non-string <null-string></null-string></td>
<td style="text-align:left">可选参数，如果没有指定，则字符串null将被使用</td>
</tr>
</tbody>
</table>

          
        
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/19/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/19/">19</a><span class="page-number current">20</span><a class="page-number" href="/page/21/">21</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><a class="extend next" rel="next" href="/page/21/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/default_avatar.jpg"
               alt="LieBrother" />
          <p class="site-author-name" itemprop="name">LieBrother</p>
          <p class="site-description motion-element" itemprop="description">当才华撑不起野心时，应该静下心来学习；当能力驾驭不了目标时，应该沉下心来历练。</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">120</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">38</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">138</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        <div class="links-of-blogroll motion-element">
          
        </div>

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script type="text/javascript" src="http://tajs.qq.com/stats?sId=55186219" charset="UTF-8"></script>
<script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script>
<script>
    const btw = new BTWPlugin();
    btw.init({
        id: 'content',
        blogId: '15497-1572145892011-295',
        name: 'LieBrother',
        qrcode: 'http://www.liebrother.com/upload/2ead36875d2647ef9f857b904cdbb3e6_.jpg',
        keyword: 'github',
    });
</script>
<div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LieBrother</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

<div>
<span id="busuanzi_container_site_uv">
  本站访客数<span id="busuanzi_value_site_uv"></span>人次
</span>
&nbsp;|&nbsp;
<span id="busuanzi_container_site_pv">
    本站总访问量<span id="busuanzi_value_site_pv"></span>次
</span>
</div>



      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  


  




<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>

  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=0.5.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=0.5.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=0.5.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"1CSH1"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
  





  
  
  

  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("eodGFnKXWTUNhQKhMktCLDNj-gzGzoHsz", "iC7dXG3oqjDHhiy0K3MmaIlE");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



</body>
</html>
